{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime\n",
    "import os,sys\n",
    "#\n",
    "import subprocess\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "#\n",
    "import urllib\n",
    "import contextlib\n",
    "# import urllib.request as request\n",
    "# from contextlib import closing\n",
    "#\n",
    "import re\n",
    "#\n",
    "import json\n",
    "import netCDF4\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NML(dict):\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "    \n",
    "    #\n",
    "    def assign(self, ky1, ky2, val):\n",
    "        # a \"safe\" assignment operator. If the keys do not exist, fail and raise an exception.\n",
    "        if ky1 in self.keys() and ky2 in self[ky1].keys():\n",
    "            self[ky1][ky2] = val\n",
    "            #\n",
    "            return True\n",
    "        else:\n",
    "            err_str = '{} not in self[{}].keys()'.format(ky2, ky1)\n",
    "            if not(ky1) in self.keys():\n",
    "                err_str = \"{} not in self.keys()\"\n",
    "            #\n",
    "            raise NameError(err_str)\n",
    "            return False\n",
    "    #\n",
    "    def nml_to_json(self, nml_in=None, json_out=None):\n",
    "        '''\n",
    "        # convert nml to dict; save dict as class member; if json_out is not None,\n",
    "        # export json to json_out\n",
    "        #\n",
    "        # NOTE: The format for .nml does not appear to be very strictly defined. Basically, it is not\n",
    "        #. well defined if key-value pairs are separated by CRLF ('\\n') or comma ',' -- oh, and also commas\n",
    "        #. are allowed (in some cases) withink values. So this might be a work in progress, to be able\n",
    "        #. to generally define k-v pairs. Right now, its success rests a little bit on, \"please, all that\n",
    "        #. is holy, let them not allow this or that...\"\n",
    "        #\n",
    "        # @nml_in: filenname of input nml\n",
    "        # @json_out: filename of output json file.\n",
    "        #. \n",
    "        '''\n",
    "        #\n",
    "        if nml_in is None:\n",
    "            nml_in = self.nml\n",
    "        #\n",
    "        #nml_dict = {}\n",
    "        group_name = ''\n",
    "        #k_group = 0\n",
    "        #\n",
    "        values_out = {}\n",
    "        ky = ''\n",
    "        \n",
    "        with open(nml_in) as fin:\n",
    "            for rw in fin:\n",
    "                rw = rw.strip()\n",
    "                #\n",
    "                # TODO: maye we should retain comments? if so, we'll need to switch the nested\n",
    "                #. structure to a list (-like), instead of a dict., to allow for multiple commented-out\n",
    "                #  entries.\n",
    "                #\n",
    "                # for a mid-line comment:\n",
    "                rw = rw.split('!')[0]\n",
    "                #\n",
    "                if len(rw)==0 or rw[0] in ['!', '\\n']:\n",
    "                    continue\n",
    "                #\n",
    "                if rw[0]=='&':\n",
    "                    # new group:\n",
    "                    if not group_name == '':\n",
    "                        self[group_name] = values_out\n",
    "                        #nml_dict[group_name]=values_out\n",
    "                    #\n",
    "                    group_name = rw[1:].strip()\n",
    "                    values_out = {}\n",
    "                    ky=''\n",
    "                    val=''\n",
    "                    #if not group_name in nml_dict.keys():\n",
    "                    #    nml_dict[group_name]={}\n",
    "                    continue\n",
    "                    #\n",
    "                \n",
    "                #\n",
    "                #print('** ', rw)\n",
    "                #\n",
    "                # TODO: this logic is almost right, and will probably work most of the time, but\n",
    "                #. it might be better to be more robust about allowing multi-line entries. Note\n",
    "                #. this will not work properly for a multi-entry line that end in a multi-line entry.\n",
    "                if rw.startswith('/'):\n",
    "                    values_out[ky]=val.strip()\n",
    "                    continue\n",
    "                #\n",
    "                ky_vl = rw.split('=')\n",
    "                if len(ky_vl) == 1:\n",
    "                    #print('*** debug: ', val, ky_vl)\n",
    "                    val = val + ky_vl[0]\n",
    "                    continue\n",
    "                elif len(ky_vl) == 2:\n",
    "                    if not ky=='':\n",
    "                        values_out[ky]=val.strip()\n",
    "                    ky,val = [s.strip() for s in ky_vl]\n",
    "                elif len(ky_vl) > 2:\n",
    "                    # there are multiple entries, presumably separated by commas?? so\n",
    "                    # key=val,key=val,key=val...\n",
    "                    #print('** DEBUG: ', [s.strip() for s in re.split('=|,', rw) if not s.strip()==''])\n",
    "                    values_out.update(dict(numpy.reshape([s.strip()\n",
    "                                        for s in re.split('=|,', rw) if not s.strip()==''], (-1,2))))\n",
    "                \n",
    "            #\n",
    "        #\n",
    "        #self.nml_dict=nml_dict\n",
    "        #self.update(nml_dict)\n",
    "        #\n",
    "        if not json_out is None:\n",
    "            with open(json_out, 'w') as fout:\n",
    "                #json.dump(nml_dict, fout)\n",
    "                # can we just dump self?\n",
    "                json.dump(self, fout)\n",
    "                #json.dump({ky:vl for ky,vl in self.items()})\n",
    "            #\n",
    "        #\n",
    "        return None\n",
    "        #return nml_dict\n",
    "    #\n",
    "    \n",
    "    #\n",
    "    def json_to_nml(self, nml_out='input.nml', json_in=None, indent=None, file_mode='w'):\n",
    "        '''\n",
    "        # convert json or dict to an nml. export to nml_out.\n",
    "        # TODO: continue to evaluate how lists, tuples, etc. are encoundered and handled. For example,\n",
    "        #  we want output to be like, format = 6,8 , not format = [6,8]. so far, i don't see any \"[]\"\n",
    "        #. characters in .nml files, so we can probably just get rid of them, but it might be smarter\n",
    "        #. to just recognize when we have a list type... or to enforce that all values are saved internally\n",
    "        #  as strings... The latter may become necessary, since there does not appear to be a good standard\n",
    "        #. for comma, space, etc. separating values (or fields).\n",
    "        '''\n",
    "        #\n",
    "        if json_in is None:\n",
    "            json_in = self\n",
    "        #\n",
    "        if indent is None:\n",
    "            indent = 4*chr(32)\n",
    "            #\n",
    "        #\n",
    "        if isinstance(json_in, str):\n",
    "            with open(json_in, 'r') as fin:\n",
    "                json_in = json.load(fin)\n",
    "            #\n",
    "        #\n",
    "        with open(nml_out, file_mode) as fout:\n",
    "            for group,entries in json_in.items():\n",
    "                fout.write('&{}\\n'.format(group))\n",
    "                #\n",
    "                for entry,val in entries.items():\n",
    "                    fout.write('{}{} = {}\\n'.format(indent, entry, val ))\n",
    "                fout.write('/\\n\\n')\n",
    "                \n",
    "                #\n",
    "            #\n",
    "            \n",
    "                    \n",
    "class NML_from_nml(NML):\n",
    "    def __init__(self, input_nml, json_out=None):\n",
    "        #\n",
    "        super(NML_from_nml,self).__init__()\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        # is this useful?\n",
    "        #with open(input_nml) as fin:\n",
    "        #    self.nml = fin.read()\n",
    "        #\n",
    "        self.nml_to_json(input_nml, json_out=json_out)\n",
    "        \n",
    "    #\n",
    "class NML_from_json(NML):\n",
    "    def __init__(self, input_json, nml_out=None):\n",
    "        #\n",
    "        super(NML_from_json,self).__init__()\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        with open(input_json) as fin:\n",
    "            #self.nml_dict = json.load(fin)\n",
    "            self.update(json.load(fin))\n",
    "        #\n",
    "        if not nml_out is None:\n",
    "            self.nml = self.json_to_nml(self.nml_json, nml_out)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** \n",
      "/Users/myoder96/Codes/AM4_runtime\n",
      "/Users/myoder96/Codes\n",
      "/Users/myoder96\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "root_dir = os.path.dirname(os.path.abspath(cwd))\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"..\"))\n",
    "#\n",
    "print('** \\n{}\\n{}\\n{}'.format(cwd, root_dir, ROOT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AM4_batch_scripter(object):\n",
    "    mpi_execs = {'mpirun':{'exec': 'mpirun', 'ntasks':'--np ', 'cpu_per_task':'-d '},\n",
    "                 'srun':{'exec':'srun', 'ntasks':'--ntasks=', 'cpu_per_task':'--cpus-per-task='}}\n",
    "    #\n",
    "    def __init__(self, batch_out='am4_batch.sh', work_dir=None, mpi_exec='mpirun',\n",
    "                 input_data_path=None, input_data_tar=None, input_data_url=None,\n",
    "                 input_nml='input_template.nml', modules=None, diag_table_src='diag_table_v101',\n",
    "                 force_copy_input=0, do_tar=0,\n",
    "                 n_tasks=48, n_threads=1, job_name='AM4_run', sbatch_options_str=''):\n",
    "        # parameters? input data file?\n",
    "        #\n",
    "        # not sure what this looks like yet, but... This script/class will be called by a wrapper\n",
    "        #.  script. This process will constitute a step in a larer script (ie, each ~2 hour run in a\n",
    "        #. twohour queue process).\n",
    "        #. script\n",
    "        # 1) review, set up, etc. the working directory\n",
    "        # 2) Are the input data there?\n",
    "        # 3) if not, are the input data availble?\n",
    "        # 4) if not, is the tar available? if not, get it; then open, then copy.\n",
    "        # 5) evaluate the input/output data. Have we achieved our objectives\n",
    "        #.  (which we've not yet defined -- runtime, etc.)? Define restart as necessary \n",
    "        #.   (*** though actually, i guess this will be done by the calling script; this script will\n",
    "        #.    just receive instructions).\n",
    "        # 6) copy diag_table nd create input.nml\n",
    "        # 7) execute MPI command\n",
    "        #\n",
    "        # for Mazama (note: the future will probabl hold am4.json files like, am4_mazama.json, \n",
    "        #. am4_sherlock3.json, etc.)\n",
    "        #\n",
    "        # current and local root path(s):\n",
    "        cwd = os.getcwd()\n",
    "        root_path = os.path.dirname(os.path.abspath(cwd))\n",
    "        default_data_path = os.path.join(root_path, 'AM4_data', 'AM4_run')\n",
    "        #\n",
    "        if input_data_path is None:\n",
    "            input_data_path = default_data_path\n",
    "        input_data_tar = (input_data_tar or os.path.join(os.path.dirname(input_data_path), 'AM4_run.tar.gz') )\n",
    "        if work_dir is None:\n",
    "            work_dir = os.path.join(cwd, 'workdir')\n",
    "        #\n",
    "        input_data_url = input_data_url or 'ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz'\n",
    "        # TODO: also download check validations:\n",
    "        # wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz.sha256\n",
    "        # wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz.sig\n",
    "        # sha256sum -c AM4_run.tar.gz.sha256\n",
    "        #gpg --verify AM4_run.tar.gz.sig\n",
    "        # \n",
    "        # TODO: How to handle the nml file? here as part of the batch, or in a wrapper that handles both?\n",
    "        #. or some combination\n",
    "        #\n",
    "        if modules is None: modules = ['intel/19', 'openmpi_3/', 'gfdl_am4/']\n",
    "        if input_data_path is None:\n",
    "            input_data_path = '{}/data/AM4_run'.format(root_path)\n",
    "        input_data_tar = input_data_tar or os.path.join(root_path, 'data', 'AM4_run.tar.gz')\n",
    "        #\n",
    "        if isinstance(mpi_exec, dict):\n",
    "            ky,vl = mpi_exec.items()[0]\n",
    "            #\n",
    "            if isinstance(vl, dict):\n",
    "                self.mpi_execs.update(mpi_exec)\n",
    "                mpi_exec = ky\n",
    "            else:\n",
    "                self.mpi_execs[mpi_exec['exec']] = mpi_exec.copy()\n",
    "                mpi_exec = mpi_exec['exec']\n",
    "            #\n",
    "        #\n",
    "        # download, untar, copy input data:\n",
    "        \n",
    "        #\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        # write an input.nml file and copy other files?\n",
    "        #\n",
    "        #\n",
    "        # write a batch file:\n",
    "        # self.write_batch_script()\n",
    "        \n",
    "    #\n",
    "    def get_input_data(self, work_dir=None, input_dir=None, input_tar=None, input_data_url=None,\n",
    "                       force_copy=None, verbose=0):\n",
    "        if work_dir is None:\n",
    "            work_dir = self.work_dir\n",
    "        force_copy = force_copy or self.force_copy_input\n",
    "        input_dir = input_dir or self.input_data_path\n",
    "        input_tar = input_tar or self.input_data_tar\n",
    "        input_data_url = input_data_url or self.input_data_url\n",
    "        #\n",
    "        # root-dir for tar file:\n",
    "        if not os.path.isdir(os.path.dirname(input_tar)):\n",
    "            os.makedirs(os.path.dirname(input_tar))\n",
    "        #\n",
    "        # It is possible we might create work_dir in advance to add some files, but not copy all the data,\n",
    "        #. so let's use INPUT as our test case.\n",
    "        # TODO: better tests to (not) copy\n",
    "        #input_dir = os.path.join(work_dir, 'INPUT')\n",
    "        #\n",
    "        print('*** DEBUG: os.path.isdir(os.path.join(work_dir, \\'INPUT\\'))={}'.format(os.path.isdir(os.path.join(work_dir, 'INPUT'))))\n",
    "        #\n",
    "       # what is a minimum file-count for INPUT? the default input config is ~500 files.\n",
    "        if not os.path.isdir(work_dir):\n",
    "            os.makedirs(work_dir)\n",
    "        if not os.path.isdir(os.path.join(work_dir, 'INPUT')) or len(os.listdir(os.path.join(work_dir, 'INPUT')))<100 or force_copy:\n",
    "            # we're going to copy data from the input source to the workdir.\n",
    "            # Do we have an input source?\n",
    "            #\n",
    "            # TODO: check for isdir(.input_path), then for .tar here.\n",
    "            if not os.path.isdir(input_dir):\n",
    "                #\n",
    "                print('*** DEBUG: os.path.isfile(input_tar)={}'.format(os.path.isfile(input_tar)))\n",
    "                if not os.path.isfile(input_tar):\n",
    "                    # TODO: consider downloading data first, then opening and writing file to mitigate\n",
    "                    #. opportunities for contamination.\n",
    "                    if verbose: print('*** VERBOSE: fetching data from {}'.format(input_data_url))\n",
    "                    #with open(input_tar, 'wb') as fout:\n",
    "                    #    fout.write(requests.get(input_data_url).content)\n",
    "                    #\n",
    "                    # NOTE: requests. does not support ftp, so use urllib or urllib2\n",
    "                    with contextlib.closing(urllib.request.urlopen(input_data_url)) as url_fin:\n",
    "                        with open(input_tar, 'wb') as fout:\n",
    "                            shutil.copyfileobj(url_fin, fout)\n",
    "                #\n",
    "                if verbose:\n",
    "                    print('*** VERBOSE: untarring * {} * to: {} '.format(input_tar, input_dir))\n",
    "                with tarfile.open(input_tar, 'r:gz') as tar:\n",
    "                    tar.extractall(path=os.path.dirname(input_dir))\n",
    "            #\n",
    "            if verbose:\n",
    "                print('*** VERBOSE: copying input data from {} to {}'.format(input_dir, work_dir))\n",
    "            #\n",
    "            #pass\n",
    "            # Nuke existing workdir? for now, let's not...\n",
    "            # TODO: it might actually be better to just use a subprocess or shell command do to this.\n",
    "            #src = input_dir\n",
    "            #dst = work_dir\n",
    "            shutil.copy2(input_dir, work_dir)\n",
    "            \n",
    "        \n",
    "        \n",
    "    #\n",
    "    def write_batch_script(self, fname_out=None):\n",
    "        '''\n",
    "        # Just as it sounds; write an AM4 batch script. use various inputs (prams, json, dicts?);\n",
    "        #. fetch, untar, copy input data as necessary, etc.\n",
    "        # Let's start by just scripting it out (mostly), and then we'll decide what to read in as\n",
    "        #. input prams, what to read in from JSON (or something), etc.\n",
    "        '''\n",
    "        #\n",
    "        fname_out = fname_out or self.batch_out\n",
    "        #\n",
    "        with open(fname_out, 'w') as fout:\n",
    "            fout.write('#!/bin/bash\\n#\\n')\n",
    "            #\n",
    "            fout.write('#SBATCH --ntasks={}'.format(self.n_tasks))\n",
    "            fout.write('#SBATCVH --ncpus_per_task={}'.format(self.n_threads))\n",
    "            if not self.jobname is None or self.jobname=='':\n",
    "                fout.write('#SBATCH --job-name={}'.format(self.jobname))\n",
    "            #\n",
    "            fout.write('#SBATCH --chdir={}'.format(work_dir) )\n",
    "            #\n",
    "            fout.write('module purge\\n#\\n')\n",
    "            #\n",
    "            for m in self.modules:\n",
    "                fout.write('module load {}\\n'.format(m))\n",
    "            #\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "    #\n",
    "def get_layouts(n_tasks=24):\n",
    "    '''\n",
    "    # compute possible layouts. Include (some) error checking for valid n_tasks?\n",
    "    '''\n",
    "    # get all integer factor pairs:\n",
    "    return numpy.array(sorted([(k,int(n_tasks/k)) for k in range(1, int(numpy.ceil(n_tasks**.5)))\n",
    "                               if n_tasks%k==0],\n",
    "                                key = lambda rw: numpy.sum(rw)))\n",
    "#\n",
    "def get_io_layouts(layout):\n",
    "    '''\n",
    "    # AM4 io_layouts. the second \"y\" term must be an integer factor of the \"y\" term of the input layout.\n",
    "    #. Don't yet understand the first term, so for now let's limit it to 1.\n",
    "    '''\n",
    "    #\n",
    "    return numpy.array(sorted([(1,int(layout[1]/k)) for k in range(1, int(numpy.ceil(layout[1]**.5)))\n",
    "                               if layout[1]%k==0], \n",
    "                              key=lambda rw:numpy.sum(rw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_out: am4_batch.sh\n",
      "work_dir: /Users/myoder96/Codes/AM4_runtime/workdir2\n",
      "mpi_exec: mpirun\n",
      "input_data_path: /Users/myoder96/Codes/AM4_data2/AM4_run\n",
      "input_data_tar: /Users/myoder96/Codes/AM4_data2/AM4_run.tar.gz\n",
      "input_data_url: ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz\n",
      "input_nml: input_template.nml\n",
      "modules: ['intel/19', 'openmpi_3/', 'gfdl_am4/']\n",
      "diag_table_src: diag_table_v101\n",
      "force_copy_input: 0\n",
      "do_tar: 0\n",
      "n_tasks: 48\n",
      "n_threads: 1\n",
      "job_name: AM4_run\n",
      "sbatch_options_str: \n",
      "cwd: /Users/myoder96/Codes/AM4_runtime\n",
      "root_path: /Users/myoder96/Codes\n",
      "default_data_path: /Users/myoder96/Codes/AM4_data/AM4_run\n"
     ]
    }
   ],
   "source": [
    "ABS = AM4_batch_scripter(input_data_path='/Users/myoder96/Codes/AM4_data2/AM4_run', \n",
    "                         work_dir='/Users/myoder96/Codes/AM4_runtime/workdir2')\n",
    "for key,val in ABS.__dict__.items():\n",
    "    print('{}: {}'.format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** DEBUG: os.path.isdir(os.path.join(work_dir, 'INPUT'))=False\n",
      "*** DEBUG: os.path.isfile(input_tar)=True\n",
      "*** VERBOSE: untarring * /Users/myoder96/Codes/AM4_data2/AM4_run.tar.gz * to: /Users/myoder96/Codes/AM4_data2/AM4_run \n"
     ]
    }
   ],
   "source": [
    "zz = ABS.get_input_data(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** coupler_nml:{'months': '1,', 'days': '0,', 'current_date': '1979,1,1,0,0,0,', 'calendar': \"'julian'\", 'dt_atmos': '1800,', 'dt_cpld': '7200,', 'use_lag_fluxes': '.true.', 'concurrent': '.false.', 'do_ocean': '.false.', 'ocean_npes': '0', 'atmos_npes': '48', 'atmos_nthreads': '1', 'use_hyper_thread': '.false.', 'ncores_per_node': '24'}\n",
      "\n",
      "*** coupler_nml:{'months': '1,', 'days': '0,', 'current_date': '1979,1,1,0,0,0,', 'calendar': \"'julian'\", 'dt_atmos': '1800,', 'dt_cpld': '7200,', 'use_lag_fluxes': '.true.', 'concurrent': '.false.', 'do_ocean': '.false.', 'ocean_npes': '0', 'atmos_npes': '48', 'atmos_nthreads': '1', 'use_hyper_thread': '.false.', 'ncores_per_node': '24'}\n",
      "\n",
      "*** vegn_data_nml:{'vegn_to_use': \"'uniform'\", 'K1': '10,', 'K2': '0.1,', 'fsc_liv': '0.9,', 'fsc_wood': '0.45,', 'c1(4)': '0.3', 'c2(4)': '0.3', 'Vmax': '2.0E-5, 2.0E-5, 2.0E-5, 2.0E-5, 1.50E-5,', 'm_cond': '4., 9., 9., 7., 7.,', 'alpha_phot': '0.05, 0.06, 0.06, 0.06, 0.06,', 'gamma_resp': '0.03, 0.02, 0.02, 0.02, 0.02,', 'tc_crit(0:2)': '3*273.16', 'fact_crit_phen(0:4)': '0., 0., 0., 0., 0.', 'fact_crit_fire(0:4)': '0., 0., 0., 0., 0.', 'cnst_crit_phen(0:4)': '0.30, 0.15, 0.15, 0.30, 0.30', 'cnst_crit_fire(0:4)': '0.15,  0.40, 0.15, 0.15, 0.15', 'wet_leaf_dreg(0:4)': '.3, .3, .3, .3, .3', 'ksi': '0, 0, 0, 0, 0,', 'leaf_refl(0:4,1)': '0.11, 0.11, 0.10, 0.10, 0.10', 'leaf_refl(0:4,2)': '0.58, 0.58, 0.45, 0.45, 0.39,', 'dat_root_zeta(0:4)': '0.35212, 0.17039, 0.28909, 0.25813, 0.17039', 'critical_root_density': '0.0,', 'tau_drip_s': '259200.0', 'cmc_lai(0:4)': '0.02, 0.02, 0.02, 0.02, 0.02', 'csc_lai(0:4)': '0.30, 0.30, 0.30, 0.30, 0.60', 'dat_snow_crit': '4*1.e7, .1', 't_transp_min': '268.', 'srl(0:1)': '112.0e3, 150.0e3', 'root_perm': '14*5e-7', 'alpha(1,3)': '4', 'leaf_age_tau(2)': '150', 'smoke_fraction': '0.9, 0.9, 0.6, 0.6, 0.6', 'tg_c3_thresh': '1', 'phen_ev1': '0.2', 'phen_ev2': '0.7', 'cmc_eps': '0.01', 'alpha(0:4,6)': '0.0, 0.0, 0.012, 0.012, 0.012', 'treefall_disturbance_rate': '0.175, 0.185, 0.025, 0.0275, 0.027'}\n",
      "\n",
      "*** simple_sulfate_nml:{'gas_conc_filename': \"'gas_conc_3D_am3p9.nc'\", 'gas_conc_time_dependency_type': \"'constant'\", 'cont_volc_source': \"'do_cont_volc'\", 'expl_volc_source': \"'do_expl_volc'\", 'aerocom_emission_filename': \"'so2_0.25_volcanoes.nc'\", 'aircraft_source': \"'do_aircraft',\", 'aircraft_filename': \"'emissions.aircraft.aero.0.5x0.5.1849-2016.nc',\", 'aircraft_emission_name(1)': \"'fuel'\", 'aircraft_time_dependency_type': \"'time_varying'\", 'aircraft_dataset_entry': '1979 1 1 0 0 0', 'so2_aircraft_EI': '0.001', 'anthro_source': \"'do_anthro',\", 'anthro_emission_name(1)': \"'so2ff',\", 'anthro_emission_name(2)': \"'so4ff',\", 'anthro_time_dependency_type': \"'time_varying'\", 'anthro_dataset_entry': '1979 1 1 0 0 0', 'anthro_filename': \"'anthro_so2.1849_2016.nc',\", 'biobur_source': \"'do_biobur',\", 'biobur_emission_name(1)': \"'so2bb',\", 'biobur_emission_name(2)': \"'so4bb',\", 'biobur_time_dependency_type': \"'time_varying'\", 'biobur_dataset_entry': '1979 1 1 0 0 0', 'biobur_filename': \"'anthro_so2.1849_2016.nc',\", 'cloud_chem_solver': '\"f1p\"', 'pH_cloud': '4.5', 'no_biobur_if_no_pbl': '.false.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NML_test = NML_from_nml('input_yoder_v101.nml')\n",
    "#\n",
    "\n",
    "# for ky,vl in JJ.nml_dict.items():\n",
    "#     print('** {}: {}'.format(ky,vl))\n",
    "\n",
    "\n",
    "for ky in ['coupler_nml','coupler_nml', 'vegn_data_nml', 'simple_sulfate_nml' ]:\n",
    "    print('*** {}:{}\\n'.format(ky, NML_test[ky]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NML_test['aerosolrad_package_nml']['sulfate_indices'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working NML example.\n",
    "\n",
    "- Start with a standard template\n",
    "- Compute layouts for an MPI configuration\n",
    "- Modify layout variables (in internal JSON/dict)\n",
    "- Export working input.nml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "n_tasks = 48\n",
    "n_threads = 1\n",
    "#\n",
    "layouts_1 = get_layouts(n_tasks)\n",
    "layouts_2 = get_layouts(n_tasks = n_tasks/6)\n",
    "#\n",
    "layout_io_1 = get_io_layouts(layouts_1[0])\n",
    "layout_io_2 = get_io_layouts(layouts_2[0])\n",
    "#\n",
    "print('** Layouts_1: ', layouts_1)\n",
    "print('** Layouts_2: ', layouts_2)\n",
    "#\n",
    "print('** Layouts_io_1: ', layout_io_1)\n",
    "print('** Layouts_io_2: ', layout_io_2)\n",
    "#\n",
    "my_nml = NML_from_nml(input_nml='input_yoder_v101.nml')\n",
    "layout_1=layouts_1[0]\n",
    "layout_2=layouts_2[0]\n",
    "#\n",
    "layout_io = layout_io_1[0]\n",
    "\n",
    "#\n",
    "# print out layouts, as they are imported:\n",
    "for grp in ('fv_core_nml', 'land_model_nml','ocean_model_nml', 'ice_model_nml'):\n",
    "    print('** {}::layout: {}'.format(grp, my_nml[grp]['layout']))\n",
    "    print('** {}::io_layout: {}'.format(grp, my_nml[grp]['layout']))\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', 48%6)\n",
    "print('** ', 48%5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('** ', my_nml['fv_core_nml']['layout'])\n",
    "#\n",
    "for grp in ('fv_core_nml', 'land_model_nml'):\n",
    "    my_nml.assign(grp, 'layout', ','.join([str(x) for x in layout_2]))\n",
    "    my_nml.assign(grp, 'io_layout', ','.join([str(x) for x in layout_io]))\n",
    "    print('** {}:: {}, {}'.format(grp, my_nml[grp]['layout'], my_nml[grp]['io_layout']))\n",
    "#\n",
    "for grp in ('ocean_model_nml', 'ice_model_nml'):\n",
    "    my_nml.assign(grp, 'layout', ','.join([str(x) for x in layout_1]))\n",
    "    my_nml.assign(grp, 'io_layout', ','.join([str(x) for x in layout_io]))\n",
    "    print('** {}:: {}, {}'.format(grp, my_nml[grp]['layout'], my_nml[grp]['io_layout']))\n",
    "#\n",
    "for ky,vl in [('npx',193), ('npy', 193), ('npz', 50)]:\n",
    "    my_nml.assign('fv_core_nml', ky, vl)\n",
    "#\n",
    "for ky,vl in [('co2_ceiling', 4800.0E-06), ('time_varying_co2', '.true.'), ('co2_base_value',348.0E-06),\n",
    "            ('co2_floor', 100.0E-06), ('c02_data_source', 'namelist') ]:\n",
    "    # NOTE: we want to allow new assignment:\n",
    "    my_nml['radiative_gases_nml'][ky] = vl\n",
    "print('*** radiative_gases: ', my_nml['radiative_gases_nml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ky,vl in my_nml['radiative_gases_nml'].items():\n",
    "    print('** {}:: {}'.format(ky, vl))\n",
    "print('\\n\\n')\n",
    "#\n",
    "for ky,vl in my_nml['fv_core_nml'].items():\n",
    "    print('** {}:: {}'.format(ky, vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nml.json_to_nml(nml_out='my_output.nml', json_in=my_nml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(','.join([str(x) for x in [1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
