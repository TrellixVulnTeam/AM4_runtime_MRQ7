{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime\n",
    "import os,sys\n",
    "#\n",
    "import subprocess\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "#\n",
    "import urllib\n",
    "import contextlib\n",
    "# import urllib.request as request\n",
    "# from contextlib import closing\n",
    "#\n",
    "import re\n",
    "#\n",
    "import json\n",
    "import netCDF4\n",
    "#\n",
    "import AM4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AM4 runtime scripting: Development, examples, and templates\n",
    "\n",
    "This notebook contains development work, examples, and templates to script AM4 simulations in Python. The basic strategy is to write a Python script to manage preliminary setup tasks, ie setting up working directories, copying data, assessing the restart status, and then (possibly) submit a simple batch script to run the large MPI job.\n",
    "\n",
    "Tasks delegated to the Python parent script include:\n",
    "\n",
    "- Do we have a working directory? Does it contain an INPUT folder?\n",
    "- If not, copy INPUT data from a designated source (which can take a long, long time).\n",
    "- Configure the batch scritp and `.nml` configuration to:\n",
    "    - Use the correct `layout` formats\n",
    "    - Request the correct number of processors\n",
    "    - Make any configuratioon changes, from a base template\n",
    "    - Other stuff too...\n",
    "- Manage restarts\n",
    "\n",
    "The advantages of this approach are:\n",
    "\n",
    "1. Python is generally considered a more versatile and easy to use scripting language than any shell language.\n",
    "2. To that point, most of these projects still come with `csh` or `tcsh` scripts, which are often simply not supported by newer OSs, including RedHat 7.x (it might be nominally available, but 1) might not be supported by admins (ie, LMOD will not work), and 2) it can in many cases just plain not run correctly. Accordingly, it is often necessary to translate any standard or provided scripts to `bash` anyway. We hope that, with these tools as a starting place, it will be easier to skip straight to Python.\n",
    "3. Shell scripting often does not directly support very simple logic, that can be very helpful when managing large, complex runs -- particularly with checkpointing. For example, floating point math is not directly supported in `bash`.\n",
    "4. Some preliminary operations, for example acquiring or copying input data, can be extremely cumbersome (it's a lot of data!); this approach allows these steps to be accomplished by a single-process setup script before the large MPI job is submitted. This will help to maintain friendships with your colleagues and sysadmins.\n",
    "5. Permits flexibility, more complex scripts, for example running in small checkpointed steps, to better utilize shared resources, and (possibly) saving the incremental output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Original code:\n",
    "# This code has been moved to AM4py.py, but it may be convenient to keep a copy here for reference.\n",
    "\n",
    "class NML(dict):\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "    \n",
    "    #\n",
    "    def assign(self, ky1, ky2, val):\n",
    "        # a \"safe\" assignment operator. If the keys do not exist, fail and raise an exception.\n",
    "        if ky1 in self.keys() and ky2 in self[ky1].keys():\n",
    "            self[ky1][ky2] = val\n",
    "            #\n",
    "            return True\n",
    "        else:\n",
    "            err_str = '{} not in self[{}].keys()'.format(ky2, ky1)\n",
    "            if not(ky1) in self.keys():\n",
    "                err_str = \"{} not in self.keys()\"\n",
    "            #\n",
    "            raise NameError(err_str)\n",
    "            return False\n",
    "    #\n",
    "    def nml_to_json(self, nml_in=None, json_out=None):\n",
    "        '''\n",
    "        # convert nml to dict; save dict as class member; if json_out is not None,\n",
    "        # export json to json_out\n",
    "        #\n",
    "        # NOTE: The format for .nml does not appear to be very strictly defined. Basically, it is not\n",
    "        #. well defined if key-value pairs are separated by CRLF ('\\n') or comma ',' -- oh, and also commas\n",
    "        #. are allowed (in some cases) withink values. So this might be a work in progress, to be able\n",
    "        #. to generally define k-v pairs. Right now, its success rests a little bit on, \"please, all that\n",
    "        #. is holy, let them not allow this or that...\"\n",
    "        #\n",
    "        # @nml_in: filenname of input nml\n",
    "        # @json_out: filename of output json file.\n",
    "        #. \n",
    "        '''\n",
    "        #\n",
    "        if nml_in is None:\n",
    "            nml_in = self.nml\n",
    "        #\n",
    "        #nml_dict = {}\n",
    "        group_name = ''\n",
    "        #k_group = 0\n",
    "        #\n",
    "        values_out = {}\n",
    "        ky = ''\n",
    "        \n",
    "        with open(nml_in) as fin:\n",
    "            for rw in fin:\n",
    "                rw = rw.strip()\n",
    "                #\n",
    "                # TODO: maye we should retain comments? if so, we'll need to switch the nested\n",
    "                #. structure to a list (-like), instead of a dict., to allow for multiple commented-out\n",
    "                #  entries.\n",
    "                #\n",
    "                # for a mid-line comment:\n",
    "                rw = rw.split('!')[0]\n",
    "                #\n",
    "                if len(rw)==0 or rw[0] in ['!', '\\n']:\n",
    "                    continue\n",
    "                #\n",
    "                if rw[0]=='&':\n",
    "                    # new group:\n",
    "                    if not group_name == '':\n",
    "                        self[group_name] = values_out\n",
    "                        #nml_dict[group_name]=values_out\n",
    "                    #\n",
    "                    group_name = rw[1:].strip()\n",
    "                    values_out = {}\n",
    "                    ky=''\n",
    "                    val=''\n",
    "                    #if not group_name in nml_dict.keys():\n",
    "                    #    nml_dict[group_name]={}\n",
    "                    continue\n",
    "                    #\n",
    "                \n",
    "                #\n",
    "                #print('** ', rw)\n",
    "                #\n",
    "                # TODO: this logic is almost right, and will probably work most of the time, but\n",
    "                #. it might be better to be more robust about allowing multi-line entries. Note\n",
    "                #. this will not work properly for a multi-entry line that end in a multi-line entry.\n",
    "                if rw.startswith('/'):\n",
    "                    values_out[ky]=val.strip()\n",
    "                    continue\n",
    "                #\n",
    "                ky_vl = rw.split('=')\n",
    "                if len(ky_vl) == 1:\n",
    "                    #print('*** debug: ', val, ky_vl)\n",
    "                    val = val + ky_vl[0]\n",
    "                    continue\n",
    "                elif len(ky_vl) == 2:\n",
    "                    if not ky=='':\n",
    "                        values_out[ky]=val.strip()\n",
    "                    ky,val = [s.strip() for s in ky_vl]\n",
    "                elif len(ky_vl) > 2:\n",
    "                    # there are multiple entries, presumably separated by commas?? so\n",
    "                    # key=val,key=val,key=val...\n",
    "                    #print('** DEBUG: ', [s.strip() for s in re.split('=|,', rw) if not s.strip()==''])\n",
    "                    values_out.update(dict(numpy.reshape([s.strip()\n",
    "                                        for s in re.split('=|,', rw) if not s.strip()==''], (-1,2))))\n",
    "                \n",
    "            #\n",
    "        #\n",
    "        #self.nml_dict=nml_dict\n",
    "        #self.update(nml_dict)\n",
    "        #\n",
    "        if not json_out is None:\n",
    "            with open(json_out, 'w') as fout:\n",
    "                #json.dump(nml_dict, fout)\n",
    "                # can we just dump self?\n",
    "                json.dump(self, fout)\n",
    "                #json.dump({ky:vl for ky,vl in self.items()})\n",
    "            #\n",
    "        #\n",
    "        return None\n",
    "        #return nml_dict\n",
    "    #\n",
    "    \n",
    "    #\n",
    "    def json_to_nml(self, nml_out='input.nml', json_in=None, indent=None, file_mode='w'):\n",
    "        '''\n",
    "        # convert json or dict to an nml. export to nml_out.\n",
    "        # TODO: continue to evaluate how lists, tuples, etc. are encoundered and handled. For example,\n",
    "        #  we want output to be like, format = 6,8 , not format = [6,8]. so far, i don't see any \"[]\"\n",
    "        #. characters in .nml files, so we can probably just get rid of them, but it might be smarter\n",
    "        #. to just recognize when we have a list type... or to enforce that all values are saved internally\n",
    "        #  as strings... The latter may become necessary, since there does not appear to be a good standard\n",
    "        #. for comma, space, etc. separating values (or fields).\n",
    "        '''\n",
    "        #\n",
    "        if json_in is None:\n",
    "            json_in = self\n",
    "        #\n",
    "        if indent is None:\n",
    "            indent = 4*chr(32)\n",
    "            #\n",
    "        #\n",
    "        if isinstance(json_in, str):\n",
    "            with open(json_in, 'r') as fin:\n",
    "                json_in = json.load(fin)\n",
    "            #\n",
    "        #\n",
    "        with open(nml_out, file_mode) as fout:\n",
    "            for group,entries in json_in.items():\n",
    "                fout.write('&{}\\n'.format(group))\n",
    "                #\n",
    "                for entry,val in entries.items():\n",
    "                    fout.write('{}{} = {}\\n'.format(indent, entry, val ))\n",
    "                fout.write('/\\n\\n')\n",
    "                \n",
    "                #\n",
    "            #\n",
    "            \n",
    "                    \n",
    "class NML_from_nml(NML):\n",
    "    def __init__(self, input_nml, output=None):\n",
    "        #\n",
    "        nml_out=output\n",
    "        super(NML_from_nml,self).__init__()\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        # is this useful?\n",
    "        #with open(input_nml) as fin:\n",
    "        #    self.nml = fin.read()\n",
    "        #\n",
    "        self.nml_to_json(input_nml, json_out=output)\n",
    "        \n",
    "    #\n",
    "class NML_from_json(NML):\n",
    "    def __init__(self, input_json, output=None):\n",
    "        #\n",
    "        json_out = output\n",
    "        super(NML_from_json,self).__init__()\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        with open(input_json) as fin:\n",
    "            #self.nml_dict = json.load(fin)\n",
    "            self.update(json.load(fin))\n",
    "        #\n",
    "        if not output is None:\n",
    "            self.nml = self.json_to_nml(self.nml_json, output)\n",
    "\n",
    "            \n",
    "\n",
    "class AM4_batch_scripter(object):\n",
    "    mpi_execs = {'mpirun':{'exec': 'mpirun', 'ntasks':'--np ', 'cpu_per_task':'-d '},\n",
    "                 'srun':{'exec':'srun', 'ntasks':'--ntasks=', 'cpu_per_task':'--cpus-per-task='}}\n",
    "    #\n",
    "    HPC_configs={\n",
    "    'mazama_hpc':{'cpu_per_node':24, 'cpu_slots':2, 'cpu_make':'intel', 'cpu_gen':'haswell',\n",
    "                  'mem_per_node':64, 'modules':['intel/19', 'openmpi_3/', 'gfdl_am4/']},\n",
    "    'sherlock2_hpc':{'cpu_per_node':24, 'cpu_slots':2, 'cpu_make':'intel', 'cpu_gen':'skylake',\n",
    "                     'mem_per_node':192},\n",
    "    'sherlock2_hpc2':{'cpu_per_node':24, 'cpu_slots':2, 'cpu_make':'intel', 'cpu_gen':'skylake',\n",
    "                      'mem_per_node':384},\n",
    "    'sherlock3_base':{'cpu_per_node':24, 'cpu_slots':1, 'cpu_make':'AMD', 'cpu_gen':'EPYC_7502',\n",
    "                      'mem_per_node':256},\n",
    "    'sherlock3_perf':{'cpu_per_node':128, 'cpu_slots':2, 'cpu_make':'AMD', 'cpu_gen':'EPYC_7742',\n",
    "                      'mem_per_node':1024}\n",
    "    }\n",
    "    #               \n",
    "    def __init__(self, batch_out='am4_batch.sh', work_dir=None, mpi_exec='mpirun',\n",
    "                 input_data_path=None, input_data_tar=None, input_data_url=None,\n",
    "                 nml_template='nml_input_template.nml', modules=None, diag_table_src='diag_table_v101',\n",
    "                 force_copy_input=0, do_tar=0, hpc_config='mazama_hpc',\n",
    "                 npes_atmos=48, nthreads_atmos=1, npes_ocean=0, job_name='AM4_run', sbatch_options_str='',\n",
    "                 copy_timeout=6000):\n",
    "        '''\n",
    "        # parameters? input data file?\n",
    "        #\n",
    "        # not sure what this looks like yet, but... This script/class will be called by a wrapper\n",
    "        #.  script. This process will constitute a step in a larer script (ie, each ~2 hour run in a\n",
    "        #. twohour queue process).\n",
    "        #. script\n",
    "        # 1) review, set up, etc. the working directory\n",
    "        # 2) Are the input data there?\n",
    "        # 3) if not, are the input data availble?\n",
    "        # 4) if not, is the tar available? if not, get it; then open, then copy.\n",
    "        # 5) evaluate the input/output data. Have we achieved our objectives\n",
    "        #.  (which we've not yet defined -- runtime, etc.)? Define restart as necessary \n",
    "        #.   (*** though actually, i guess this will be done by the calling script; this script will\n",
    "        #.    just receive instructions).\n",
    "        # 6) copy diag_table nd create input.nml\n",
    "        # 7) execute MPI command\n",
    "        #\n",
    "        # for Mazama (note: the future will probabl hold am4.json files like, am4_mazama.json, \n",
    "        #. am4_sherlock3.json, etc.)\n",
    "        #\n",
    "        # current and local root path(s):\n",
    "        #\n",
    "        # @n_tasks: I believe n_tasks needs to be an integer multiple of 6, ie 6 faces to a cube.\n",
    "        # (so any threads work on a single task/cube face). For now, let's stick with that, particularly\n",
    "        #. since we'll probably usually use nthreads=1\n",
    "        #. \n",
    "        '''\n",
    "        #        #\n",
    "        if isinstance(hpc_config, str):\n",
    "            if hpc_config.endswith('.json'):\n",
    "                hpc_config=json.load(hpc_config)\n",
    "            else:\n",
    "                hpc_config=self.HPC_configs[hpc_config]\n",
    "            #\n",
    "        # else hpc_config better be a dict of HW configs.\n",
    "        #\n",
    "        # \n",
    "        if npes_atmos%6 != 0:\n",
    "            npes_atmos += npes_atmos%6\n",
    "            print('*** WARNING: npes_atmos must be a multiple of 6. Increasing tasks -> {} so that npes_atmos%6=0'.format(npes_atmosn_tasks))\n",
    "        #\n",
    "        # same for ocean? npes_ocean%6==0 ??? right now, we're not doing ocean, so...\n",
    "        # TODO: we can also check to see that we don't ask for more threads than we can get, but since\n",
    "        #. these will almost always be single-threaded, let's hold off...\n",
    "        #\n",
    "        cwd = os.getcwd()\n",
    "        root_path = os.path.dirname(os.path.abspath(cwd))\n",
    "        default_data_path = os.path.join(root_path, 'AM4_data', 'AM4_run')\n",
    "        #\n",
    "        if input_data_path is None:\n",
    "            input_data_path = default_data_path\n",
    "        input_data_tar = (input_data_tar or os.path.join(os.path.dirname(input_data_path), 'AM4_run.tar.gz') )\n",
    "        if work_dir is None:\n",
    "            work_dir = os.path.join(cwd, 'workdir')\n",
    "        #\n",
    "        input_data_url = input_data_url or 'ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz'\n",
    "        # TODO: also download check validations:\n",
    "        # wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz.sha256\n",
    "        # wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz.sig\n",
    "        # sha256sum -c AM4_run.tar.gz.sha256\n",
    "        #gpg --verify AM4_run.tar.gz.sig\n",
    "        # \n",
    "        # TODO: How to handle the nml file? here as part of the batch, or in a wrapper that handles both?\n",
    "        #. or some combination\n",
    "        #\n",
    "        if modules is None:\n",
    "            modules = hpc_config.get('modules', ['intel/19', 'openmpi_3/', 'gfdl_am4/'])\n",
    "            #modules = ['intel/19', 'openmpi_3/', 'gfdl_am4/']\n",
    "        #\n",
    "        if input_data_path is None:\n",
    "            input_data_path = '{}/data/AM4_run'.format(root_path)\n",
    "        input_data_tar = input_data_tar or os.path.join(root_path, 'data', 'AM4_run.tar.gz')\n",
    "        #\n",
    "        if isinstance(mpi_exec, dict):\n",
    "            ky,vl = mpi_exec.items()[0]\n",
    "            #\n",
    "            # multi-category dict?\n",
    "            if isinstance(vl, dict):\n",
    "                self.mpi_execs.update(mpi_exec)\n",
    "                mpi_exec = vl\n",
    "            else:\n",
    "                # single-entry dict; add this full mpi_exec entry to mpi_execs\n",
    "                ky = mpi_exec['exec']\n",
    "                self.mpi_execs[ky] = mpi_exec.copy()\n",
    "            #\n",
    "        else:\n",
    "            mpi_exec = self.mpi_execs[mpi_exec]\n",
    "        #\n",
    "        #\n",
    "        # download, untar, copy input data:\n",
    "        \n",
    "        #\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        layout_1 = get_layouts(self.ntasks_total)[0]\n",
    "        layout_io_1 = get_io_layouts(layout_1)[0]\n",
    "        layout_2 = get_layouts(self.ntasks_total/6)[0]\n",
    "        layout_io_2 = get_io_layouts(layout_2)[0]\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in self.__dict__.keys()})\n",
    "        \n",
    "        # write an input.nml file and copy other files?\n",
    "        #\n",
    "        #\n",
    "        # write a batch file:\n",
    "        # self.write_batch_script()\n",
    "        \n",
    "    #\n",
    "    # NOTE: these nvcpu calculators will likely not work well for not-simple configurations,\n",
    "    #. but again, I think most of these jobs will be single-threaded.\n",
    "    @property\n",
    "    def cpus_total(self):\n",
    "        # NOTE: this is cpus, but not tasks. it should tell us how many cores we'll be expecting,\n",
    "        #. but probably is not something submitted to SLURM unless there are constraints on nodes.\n",
    "        return self.npes_atmos * self.nthreads_atmos + self.npes_ocean\n",
    "    @property\n",
    "    def ntasks_total(self):\n",
    "        return self.npes_atmos + self.npes_ocean\n",
    "    @property \n",
    "    def n_tasks(self):\n",
    "        return self.ntasks_total\n",
    "    @property\n",
    "    def n_threads(self):\n",
    "        return self.nthreads_atmos\n",
    "    #\n",
    "    #def layout_to_str(self, layout):\n",
    "    #    return '{},{}'.format(*layout)\n",
    "    #\n",
    "    @property\n",
    "    def default_nml_reconfig(self):\n",
    "        # TODO: check these. if we're actually using ocean... is ocean supposet to be n*x=npes_ocean?\n",
    "        #.  npes_total???\n",
    "#         return {'coupler_nml':{'atmos_npes':self.npes_atmos, 'atmos_nthreads':self.nthreads_atmos,\n",
    "#                               'ocean_npes':self.npes_ocean},\n",
    "#                'fv_core_nml':{'layout':self.layout_2, 'io_layout':self.layout_io_2},\n",
    "#                'ice_model_nml':{'layout':self.layout_1, 'io_layout':self.layout_io_1},\n",
    "#                'land_model_nml':{'layout':self.layout_2, 'io_layout':self.layout_io_2},\n",
    "#                'ocean_model_nml':{'layout':self.layout_1, 'io_layout':self.layout_io_1},\n",
    "#                }\n",
    "        return {'coupler_nml':{'atmos_npes':self.npes_atmos, 'atmos_nthreads':self.nthreads_atmos,\n",
    "                              'ocean_npes':self.npes_ocean},\n",
    "               'fv_core_nml':{'layout':'{},{}'.format(*self.layout_2),\n",
    "                              'io_layout':'{},{}'.format(*self.layout_io_2)},\n",
    "               'ice_model_nml':{'layout':'{},{}'.format(*self.layout_1),\n",
    "                              'io_layout':'{},{}'.format(*self.layout_io_1)},\n",
    "               'land_model_nml':{'layout':'{},{}'.format(*self.layout_2),\n",
    "                              'io_layout':'{},{}'.format(*self.layout_io_2)},\n",
    "               'ocean_model_nml':{'layout':'{},{}'.format(*self.layout_1),\n",
    "                              'io_layout':'{},{}'.format(*self.layout_io_1)},\n",
    "                'atmos_model_nml':{'nxblocks':1, 'nyblocks':self.nthreads_atmos}\n",
    "               }\n",
    "    #\n",
    "    def get_input_data(self, work_dir=None, input_dir=None, input_tar=None, input_data_url=None,\n",
    "                       force_copy=None, verbose=0, copy_timeout=None, diag_table_src=None):\n",
    "        if work_dir is None:\n",
    "            work_dir = self.work_dir\n",
    "        force_copy = force_copy or self.force_copy_input\n",
    "        input_dir = input_dir or self.input_data_path\n",
    "        input_tar = input_tar or self.input_data_tar\n",
    "        input_data_url = input_data_url or self.input_data_url\n",
    "        copy_timeout = copy_timeout or self.copy_timeout\n",
    "        diag_table_src = diag_table_src or self.diag_table_src\n",
    "        #\n",
    "        # root-dir for tar file:\n",
    "        if not os.path.isdir(os.path.dirname(input_tar)):\n",
    "            os.makedirs(os.path.dirname(input_tar))\n",
    "        #\n",
    "        # It is possible we might create work_dir in advance to add some files, but not copy all the data,\n",
    "        #. so let's use INPUT as our test case.\n",
    "        # TODO: better tests to (not) copy\n",
    "        #input_dir = os.path.join(work_dir, 'INPUT')\n",
    "        #\n",
    "        print('*** DEBUG: os.path.isdir(os.path.join(work_dir, \\'INPUT\\'))={}'.format(os.path.isdir(os.path.join(work_dir, 'INPUT'))))\n",
    "        #\n",
    "       # what is a minimum file-count for INPUT? the default input config is ~500 files.\n",
    "        if not os.path.isdir(work_dir):\n",
    "            os.makedirs(work_dir)\n",
    "        #\n",
    "        if not os.path.isdir(os.path.join(work_dir, 'INPUT')) or len(os.listdir(os.path.join(work_dir, 'INPUT')))<100 or force_copy:\n",
    "            # work_dir is not there or not complete, so\n",
    "            # we're going to copy data from the input source to the workdir.\n",
    "            # Do we have an input source?\n",
    "            #\n",
    "            # TODO: check for isdir(.input_path), then for .tar here.\n",
    "            if not os.path.isdir(input_dir):\n",
    "                # input data are not available; go get them!\n",
    "                #\n",
    "                if verbose:\n",
    "                    print('*** VERBOSE: os.path.isfile(input_tar)={}'.format(os.path.isfile(input_tar)))\n",
    "                if not os.path.isfile(input_tar):\n",
    "                    # No TAR file avilble; we'll have to get it...\n",
    "                    # TODO: consider downloading data first, then opening and writing file to mitigate\n",
    "                    #. opportunities for contamination.\n",
    "                    if verbose: print('*** VERBOSE: fetching data from {}'.format(input_data_url))\n",
    "                    #\n",
    "                    # NOTE: requests. does not support ftp, so use urllib or urllib2\n",
    "                    with contextlib.closing(urllib.request.urlopen(input_data_url)) as url_fin:\n",
    "                        with open(input_tar, 'wb') as fout:\n",
    "                            shutil.copyfileobj(url_fin, fout)\n",
    "                #\n",
    "                if verbose:\n",
    "                    print('*** VERBOSE: untarring * {} * to: {} '.format(input_tar, input_dir))\n",
    "                with tarfile.open(input_tar, 'r:gz') as tar:\n",
    "                    # this is another one of those cases where a shell call might be easier...\n",
    "                    tar.extractall(path=os.path.dirname(input_dir))\n",
    "            #\n",
    "            if verbose:\n",
    "                print('*** VERBOSE: copying input data from {} to {}'.format(input_dir, work_dir))\n",
    "            #\n",
    "            # Nuke existing workdir? for now, let's not...\n",
    "            # TODO: shutil.copy2() should work for this, but it's being tempermental, maybe about the\n",
    "            #. directory. So for now, let's just subprocess the shell command and revisit later. It could\n",
    "            #. be a versioing problem (subprocess and related libraries are pretty version squirrelie).\n",
    "            #src = input_dir\n",
    "            #dst = work_dir\n",
    "            # TODO: The Python copy options just don't seem to work very well (maybe there's a recursive option command or \n",
    "            #. something to tell it we're copying a directory?). Let's just use a shell command.\n",
    "            #shutil.copy2(input_dir, work_dir)\n",
    "            cp_command = 'cp -rf {}/ {}/'.format(input_dir, work_dir)\n",
    "            try:\n",
    "                # NOTE: beware the timeout.... I think the default timeout is 60 seconds or something;\n",
    "                #. this can take a while, so I've been using 6000 seconds as a default.\n",
    "                #sp_output = subprocess.run(cp_command.split(chr(32)), check=True,\n",
    "                #           stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "                sp_output = subprocess.run(cp_command.split(chr(32)), check=True,\n",
    "                           capture_output=True, timeout=copy_timeout)\n",
    "            except:\n",
    "                #print('*** copy error: stdout: {}, sterr: {}'.format(sp_output.stdout, sp_output=stderr) )\n",
    "                print('*** COPY ERROR: {}'.format(cp_command))\n",
    "                raise Exception('subprocess, copy error.')\n",
    "            #\n",
    "            if verbose:\n",
    "                print('*** VERBOSE: copy input data complete, from {} to {}'.format(input_dir, work_dir))\n",
    "            #\n",
    "            # copy diag_table from a source file. We're going to construct an input.nml separately.\n",
    "            # NOTE: Again, might be better to do this as a shell command, for a number of reasons.\n",
    "            shutil.copy(diag_table_src, os.path.append(work_dir, 'diag_table'))\n",
    "            #\n",
    "        if not os.path.isdir(os.path.join(work_dir, 'RESTART')):\n",
    "            os.makedirs(os.path.join(work_dir, 'RESTART'))\n",
    "        #\n",
    "    #\n",
    "    def make_NML(self, nml_template=None, nml_configs=[], nml_out=None, json_out=None):\n",
    "        '''\n",
    "        # Some logic to managing and writing nml output.\n",
    "        # @nml_template: An nml file or .json as a starter place.\n",
    "        # @ nml_configs: 1 or more dicts (allow json filenames??) with configuration(s); they will be\n",
    "        #.  integrated, in order, via dict.update(nml_config)\n",
    "        #\n",
    "        '''\n",
    "        # add any default configs:\n",
    "        # NOTE: putting default_nml_reconfig first allows the user to override.\n",
    "        nml_configs = [self.default_nml_reconfig] + list(nml_configs)\n",
    "        nml_template = nml_template or self.nml_template\n",
    "        \n",
    "        #\n",
    "        if nml_template.endswith('.nml'):\n",
    "            f_handler = NML_from_nml\n",
    "            #NML = NML_from_nml(nml_template, output=None)\n",
    "        elif nml_template.endswith('.json'):\n",
    "            #NML = NML_from_json(nml_template, output=None)\n",
    "            f_handler = NML_from_json\n",
    "            \n",
    "        else:\n",
    "            # guess?\n",
    "            # TODO: crack it open and look for .nml or\n",
    "            #NML = NML_from_nml(nml_template, json_out=None)\n",
    "            with open(nml_template, 'r') as fin:\n",
    "                f_handler = NML_from_json\n",
    "                for rw in fin:\n",
    "                    if rw.startswith('&coupler_nml'):\n",
    "                        # it is probably a .nml file.\n",
    "                        f_handler = NML_from_nml\n",
    "                        break\n",
    "                    #\n",
    "                #\n",
    "        NML = f_handler(nml_template, None)\n",
    "            #\n",
    "        #\n",
    "        for config in nml_configs:\n",
    "            #\n",
    "            if isinstance(config, str):\n",
    "                with open(config, 'r') as fin:\n",
    "                    config = json.load(config)\n",
    "                #\n",
    "            #\n",
    "            #NML.update(config)\n",
    "            for ky,d in config.items():\n",
    "                NML[ky].update(d)\n",
    "            #\n",
    "        #\n",
    "        if not nml_out is None:\n",
    "            NML.json_to_nml(nml_out=nml_out)\n",
    "        if not json_out is None:\n",
    "            with open(json_out, 'w') as fout:\n",
    "                json.dump(NML, fout)\n",
    "            #\n",
    "        self.NML=NML\n",
    "        #\n",
    "        return NML  \n",
    "    #\n",
    "    def write_batch_script(self, fname_out=None, chdir=None, output_out=None, output_err=None,\n",
    "                           mpi_exec=None):\n",
    "        '''\n",
    "        # Just as it sounds; write an AM4 batch script. use various inputs (prams, json, dicts?);\n",
    "        #. fetch, untar, copy input data as necessary, etc.\n",
    "        # Let's start by just scripting it out (mostly), and then we'll decide what to read in as\n",
    "        #. input prams, what to read in from JSON (or something), etc. Note that, for now, this will\n",
    "        #  focus on a SE3 Mazama build/configuration.\n",
    "        '''\n",
    "        #\n",
    "        fname_out = fname_out or os.path.join(self.work_dir, self.batch_out)\n",
    "        chdir = chdir or self.work_dir\n",
    "        output_out = output_out or 'AM4_out_%j.out'\n",
    "        output_err = output_err or 'AM4_out_%j.err'\n",
    "        mpi_exec = mpi_exec or self.mpi_exec\n",
    "        #\n",
    "        with open(fname_out, 'w') as fout:\n",
    "            # This section should be pretty universal:\n",
    "            fout.write('#!/bin/bash\\n#\\n')\n",
    "            #\n",
    "            fout.write('#SBATCH --ntasks={}\\n'.format(self.n_tasks))\n",
    "            fout.write('#SBATCVH --ncpus_per_task={}\\n'.format(self.nthreads_atmos))\n",
    "            if not self.job_name is None or self.job_name=='':\n",
    "                fout.write('#SBATCH --job-name={}\\n'.format(self.job_name))\n",
    "            #\n",
    "            fout.write('#SBATCH --chdir={}\\n'.format(chdir) )\n",
    "            fout.write('#SBATCH --output={}\\n'.format(output_out))\n",
    "            fout.write('#SBATCH --error={}\\n'.format(output_err))\n",
    "            #\n",
    "            # Module swill be platform dependent. We could handle this section with JSON\n",
    "            #. if we wanted to.\n",
    "            fout.write('#\\nmodule purge\\n#\\n')\n",
    "            #\n",
    "            for m in self.modules:\n",
    "                fout.write('module load {}\\n'.format(m))\n",
    "            #\n",
    "            fout.write('#\\n#\\n')\n",
    "            # Universal??? My guess is we can impprove performance by increasing these values on bigger\n",
    "            #. memory machines.\n",
    "            # these probably need to be parameterized somehow, or included in a template... or otherwise\n",
    "            #. we need to figure out what they should be. I just copied them from GFDL sample scripts.\n",
    "            fout.write('export KMP_STACKSIZE=512m\\n')\n",
    "            fout.write('export NC_BLKSZ=1M\\n')\n",
    "            fout.write('export F_UFMTENDIAN=big\\n')\n",
    "            #\n",
    "            # executable will be pretty system/build dependent\n",
    "            fout.write('EXECUTABLE=${AM4_GFDL_BIN}/${AM4_GFDL_EXE}\\n')\n",
    "            #\n",
    "            # We can probably make this part pretty universal as well...\n",
    "            fout.write('#\\nulimit -s unlimited\\n#\\n')\n",
    "            fout.write('#\\ncd {}\\n#\\n'.format(self.work_dir))\n",
    "            fout.write('MPI_COMMAND={} {}{} {}{} ${{EXECUTABLE}}\\n#\\n'.format(mpi_exec['exec'],\n",
    "                                                            mpi_exec['ntasks'], self.n_tasks,\n",
    "                                                            mpi_exec['cpu_per_task'], self.n_threads))\n",
    "            #\n",
    "            # add an error-check:\n",
    "            for ln in ['if [[ %? -ne 0 ]]; then', 'echo \"ERROR: Run failed.\" 1>&2', \n",
    "                       '\"ERROR: Output from run in {}/fms.out ... or maybe in a log file\" 1>&2'.format(self.work_dir),\n",
    "                       'exit 1', 'fi' ]:\n",
    "                fout.write('{}\\n'.format(ln))\n",
    "            #\n",
    "        #\n",
    "            \n",
    "    #\n",
    "def get_layouts(n_tasks=24):\n",
    "    '''\n",
    "    # compute possible layouts. Include (some) error checking for valid n_tasks?\n",
    "    '''\n",
    "    # get all integer factor pairs:\n",
    "    return numpy.array(sorted([(k,int(n_tasks/k)) for k in range(1, int(numpy.ceil(n_tasks**.5)))\n",
    "                               if n_tasks%k==0],\n",
    "                                key = lambda rw: numpy.sum(rw)))\n",
    "#\n",
    "def get_io_layouts(layout):\n",
    "    '''\n",
    "    # AM4 io_layouts. the second \"y\" term must be an integer factor of the \"y\" term of the input layout.\n",
    "    #. Don't yet understand the first term, so for now let's limit it to 1.\n",
    "    '''\n",
    "    #\n",
    "    return numpy.array(sorted([(1,int(layout[1]/k)) for k in range(1, int(numpy.ceil(layout[1]**.5)))\n",
    "                               if layout[1]%k==0], \n",
    "                              key=lambda rw:numpy.sum(rw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AM4py Example\n",
    "\n",
    "- OPTIONAL:\n",
    "    - Define big-picture (ie, multiple runs) job\n",
    "    - Evaluate `work_dir` data to determine next actions (ie, did the previous job run, are we finished, etc.)\n",
    "- Instantiate an `AM4_batch_scripter` object\n",
    "    - It will evaluate whether the input data exist; if not, it will go get it.\n",
    "- Modify the NML configuration and export a working NML\n",
    "- Write a batch file to run AM4\n",
    "- Submit that script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/myoder96\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** input_data_path:  /Users/myoder96/Codes/AM4_data2/AM4_run\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "input_data_path = os.path.join(os.environ['HOME'], 'Codes', 'AM4_data2', 'AM4_run')\n",
    "work_dir = os.path.join(os.environ['HOME'], 'Codes', 'AM4_runtime', 'workdir2')\n",
    "job_name = 'AM4_dev'\n",
    "batch_job_name = os.path.join(work_dir, 'AM4_batch_example.bs')\n",
    "#\n",
    "#print('*** input_data_path: ', input_data_path)\n",
    "#\n",
    "ABS = AM4py.AM4_batch_scripter(input_data_path=imput_data_path, work_dir=work_dir,\n",
    "                               job_name='AM4_dev', batch_out=batch_job_name )\n",
    "#\n",
    "print('*** ABS variables:')\n",
    "for key,val in ABS.__dict__.items():\n",
    "    print('{}: {}'.format(key,val))\n",
    "#\n",
    "\n",
    "zz = ABS.get_input_data(verbose=True)\n",
    "\n",
    "\n",
    "my_configs = {'coupler_nml':{'days':10, 'months':0}, 'fv_core_nml':{'npx':193, 'npy':193, 'npz':50}}\n",
    "my_nml = ABS.make_NML(nml_template='input_yoder_v101.nml', nml_configs=[my_configs],\n",
    "                      nml_out=os.path.join(ABS.work_dir, 'input.nml') )\n",
    "# NML_from_nml('input_yoder_v101.nml')\n",
    "\n",
    "#my_nml = NML_from_nml('input_yoder_v101.nml')\n",
    "print('** my_nml[fv_core_nml]:' )\n",
    "print('** ', my_nml['fv_core_nml'])\n",
    "#print(my_nml.keys())\n",
    "\n",
    "print('** batch_out: ', ABS.batch_out)\n",
    "#\n",
    "ABS.write_batch_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  am4_batch.sh\n"
     ]
    }
   ],
   "source": [
    "print('** ', ABS.batch_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exec': 'mpirun', 'ntasks': '--np ', 'cpu_per_task': '-d '}\n",
      "MPI_COMMAND=mpirun --np 48 -d 1 ${EXECUTABLE\\}\n"
     ]
    }
   ],
   "source": [
    "print(ABS.mpi_exec)\n",
    "\n",
    "print('MPI_COMMAND={} {}{} {}{} ${{EXECUTABLE\\}}'.format(ABS.mpi_exec['exec'],\n",
    "                                                            ABS.mpi_exec['ntasks'], ABS.n_tasks,\n",
    "                                                            ABS.mpi_exec['cpu_per_task'], ABS.n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified syntax to capture both stdout and stderr (to separate outputs?). NOTE: this syntax has been\n",
    "#. a moving target\n",
    "#sp_output = subprocess.run('ls -lh'.split(chr(32)), check=True, capture_output=True)\n",
    "#\n",
    "# pipe stdout and stderr to the same output\n",
    "sp_output = subprocess.run('ls -lh /Users/myoder96/Codes/AM4_data2/AM4_run/'.split(chr(32)), check=True,\n",
    "                           stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "#\n",
    "print('** sp_output: \\n', sp_output)\n",
    "# for rw in sp_output.stdout.decode().split('\\n'):\n",
    "#     print*('** ', rw)\n",
    "print('** ** stdout: \\n', sp_output.stdout.decode())\n",
    "for rw in sp_output.stdout.decode().split('\\n'):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NML_test = NML_from_nml('input_yoder_v101.nml')\n",
    "#\n",
    "\n",
    "# for ky,vl in JJ.nml_dict.items():\n",
    "#     print('** {}: {}'.format(ky,vl))\n",
    "\n",
    "\n",
    "for ky in ['coupler_nml','coupler_nml', 'vegn_data_nml', 'simple_sulfate_nml' ]:\n",
    "    print('*** {}:{}\\n'.format(ky, NML_test[ky]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NML_test['aerosolrad_package_nml']['sulfate_indices'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working NML example.\n",
    "\n",
    "- Start with a standard template\n",
    "- Compute layouts for an MPI configuration\n",
    "- Modify layout variables (in internal JSON/dict)\n",
    "- Export working input.nml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "n_tasks = 48\n",
    "n_threads = 1\n",
    "#\n",
    "layouts_1 = get_layouts(n_tasks)\n",
    "layouts_2 = get_layouts(n_tasks = n_tasks/6)\n",
    "#\n",
    "layout_io_1 = get_io_layouts(layouts_1[0])\n",
    "layout_io_2 = get_io_layouts(layouts_2[0])\n",
    "#\n",
    "print('** Layouts_1: ', layouts_1)\n",
    "print('** Layouts_2: ', layouts_2)\n",
    "#\n",
    "print('** Layouts_io_1: ', layout_io_1)\n",
    "print('** Layouts_io_2: ', layout_io_2)\n",
    "#\n",
    "my_nml = NML_from_nml(input_nml='input_yoder_v101.nml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** ', my_nml.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_1=layouts_1[0]\n",
    "layout_2=layouts_2[0]\n",
    "#\n",
    "layout_io = layout_io_1[0]\n",
    "\n",
    "#\n",
    "# print out layouts, as they are imported:\n",
    "for grp in ('fv_core_nml', 'land_model_nml','ocean_model_nml', 'ice_model_nml'):\n",
    "    print('** {}::layout: {}'.format(grp, my_nml[grp]['layout']))\n",
    "    print('** {}::io_layout: {}'.format(grp, my_nml[grp]['layout']))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', 48%6)\n",
    "print('** ', 48%5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('** ', my_nml['fv_core_nml']['layout'])\n",
    "#\n",
    "for grp in ('fv_core_nml', 'land_model_nml'):\n",
    "    my_nml.assign(grp, 'layout', ','.join([str(x) for x in layout_2]))\n",
    "    my_nml.assign(grp, 'io_layout', ','.join([str(x) for x in layout_io]))\n",
    "    print('** {}:: {}, {}'.format(grp, my_nml[grp]['layout'], my_nml[grp]['io_layout']))\n",
    "#\n",
    "for grp in ('ocean_model_nml', 'ice_model_nml'):\n",
    "    my_nml.assign(grp, 'layout', ','.join([str(x) for x in layout_1]))\n",
    "    my_nml.assign(grp, 'io_layout', ','.join([str(x) for x in layout_io]))\n",
    "    print('** {}:: {}, {}'.format(grp, my_nml[grp]['layout'], my_nml[grp]['io_layout']))\n",
    "#\n",
    "for ky,vl in [('npx',193), ('npy', 193), ('npz', 50)]:\n",
    "    my_nml.assign('fv_core_nml', ky, vl)\n",
    "#\n",
    "for ky,vl in [('co2_ceiling', 4800.0E-06), ('time_varying_co2', '.true.'), ('co2_base_value',348.0E-06),\n",
    "            ('co2_floor', 100.0E-06), ('c02_data_source', 'namelist') ]:\n",
    "    # NOTE: we want to allow new assignment:\n",
    "    my_nml['radiative_gases_nml'][ky] = vl\n",
    "print('*** radiative_gases: ', my_nml['radiative_gases_nml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ky,vl in my_nml['radiative_gases_nml'].items():\n",
    "    print('** {}:: {}'.format(ky, vl))\n",
    "print('\\n\\n')\n",
    "#\n",
    "for ky,vl in my_nml['fv_core_nml'].items():\n",
    "    print('** {}:: {}'.format(ky, vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nml.json_to_nml(nml_out='my_output.nml', json_in=my_nml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(','.join([str(x) for x in [1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
