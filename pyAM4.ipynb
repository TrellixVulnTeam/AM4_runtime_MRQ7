{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mpd\n",
    "import pylab as plt\n",
    "import datetime\n",
    "import os,sys\n",
    "#\n",
    "import subprocess\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "#\n",
    "import urllib\n",
    "import contextlib\n",
    "# import urllib.request as request\n",
    "# from contextlib import closing\n",
    "#\n",
    "import re\n",
    "#\n",
    "import json\n",
    "import netCDF4\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NML(dict):\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "    \n",
    "    #\n",
    "    def assign(self, ky1, ky2, val):\n",
    "        # a \"safe\" assignment operator. If the keys do not exist, fail and raise an exception.\n",
    "        if ky1 in self.keys() and ky2 in self[ky1].keys():\n",
    "            self[ky1][ky2] = val\n",
    "            #\n",
    "            return True\n",
    "        else:\n",
    "            err_str = '{} not in self[{}].keys()'.format(ky2, ky1)\n",
    "            if not(ky1) in self.keys():\n",
    "                err_str = \"{} not in self.keys()\"\n",
    "            #\n",
    "            raise NameError(err_str)\n",
    "            return False\n",
    "    #\n",
    "    def nml_to_json(self, nml_in=None, json_out=None):\n",
    "        '''\n",
    "        # convert nml to dict; save dict as class member; if json_out is not None,\n",
    "        # export json to json_out\n",
    "        #\n",
    "        # NOTE: The format for .nml does not appear to be very strictly defined. Basically, it is not\n",
    "        #. well defined if key-value pairs are separated by CRLF ('\\n') or comma ',' -- oh, and also commas\n",
    "        #. are allowed (in some cases) withink values. So this might be a work in progress, to be able\n",
    "        #. to generally define k-v pairs. Right now, its success rests a little bit on, \"please, all that\n",
    "        #. is holy, let them not allow this or that...\"\n",
    "        #\n",
    "        # @nml_in: filenname of input nml\n",
    "        # @json_out: filename of output json file.\n",
    "        #. \n",
    "        '''\n",
    "        #\n",
    "        if nml_in is None:\n",
    "            nml_in = self.nml\n",
    "        #\n",
    "        #nml_dict = {}\n",
    "        group_name = ''\n",
    "        #k_group = 0\n",
    "        #\n",
    "        values_out = {}\n",
    "        ky = ''\n",
    "        \n",
    "        with open(nml_in) as fin:\n",
    "            for rw in fin:\n",
    "                rw = rw.strip()\n",
    "                #\n",
    "                # TODO: maye we should retain comments? if so, we'll need to switch the nested\n",
    "                #. structure to a list (-like), instead of a dict., to allow for multiple commented-out\n",
    "                #  entries.\n",
    "                #\n",
    "                # for a mid-line comment:\n",
    "                rw = rw.split('!')[0]\n",
    "                #\n",
    "                if len(rw)==0 or rw[0] in ['!', '\\n']:\n",
    "                    continue\n",
    "                #\n",
    "                if rw[0]=='&':\n",
    "                    # new group:\n",
    "                    if not group_name == '':\n",
    "                        self[group_name] = values_out\n",
    "                        #nml_dict[group_name]=values_out\n",
    "                    #\n",
    "                    group_name = rw[1:].strip()\n",
    "                    values_out = {}\n",
    "                    ky=''\n",
    "                    val=''\n",
    "                    #if not group_name in nml_dict.keys():\n",
    "                    #    nml_dict[group_name]={}\n",
    "                    continue\n",
    "                    #\n",
    "                \n",
    "                #\n",
    "                #print('** ', rw)\n",
    "                #\n",
    "                # TODO: this logic is almost right, and will probably work most of the time, but\n",
    "                #. it might be better to be more robust about allowing multi-line entries. Note\n",
    "                #. this will not work properly for a multi-entry line that end in a multi-line entry.\n",
    "                if rw.startswith('/'):\n",
    "                    values_out[ky]=val.strip()\n",
    "                    continue\n",
    "                #\n",
    "                ky_vl = rw.split('=')\n",
    "                if len(ky_vl) == 1:\n",
    "                    #print('*** debug: ', val, ky_vl)\n",
    "                    val = val + ky_vl[0]\n",
    "                    continue\n",
    "                elif len(ky_vl) == 2:\n",
    "                    if not ky=='':\n",
    "                        values_out[ky]=val.strip()\n",
    "                    ky,val = [s.strip() for s in ky_vl]\n",
    "                elif len(ky_vl) > 2:\n",
    "                    # there are multiple entries, presumably separated by commas?? so\n",
    "                    # key=val,key=val,key=val...\n",
    "                    #print('** DEBUG: ', [s.strip() for s in re.split('=|,', rw) if not s.strip()==''])\n",
    "                    values_out.update(dict(numpy.reshape([s.strip()\n",
    "                                        for s in re.split('=|,', rw) if not s.strip()==''], (-1,2))))\n",
    "                \n",
    "            #\n",
    "        #\n",
    "        #self.nml_dict=nml_dict\n",
    "        #self.update(nml_dict)\n",
    "        #\n",
    "        if not json_out is None:\n",
    "            with open(json_out, 'w') as fout:\n",
    "                #json.dump(nml_dict, fout)\n",
    "                # can we just dump self?\n",
    "                json.dump(self, fout)\n",
    "                #json.dump({ky:vl for ky,vl in self.items()})\n",
    "            #\n",
    "        #\n",
    "        return None\n",
    "        #return nml_dict\n",
    "    #\n",
    "    \n",
    "    #\n",
    "    def json_to_nml(self, nml_out='input.nml', json_in=None, indent=None, file_mode='w'):\n",
    "        '''\n",
    "        # convert json or dict to an nml. export to nml_out.\n",
    "        # TODO: continue to evaluate how lists, tuples, etc. are encoundered and handled. For example,\n",
    "        #  we want output to be like, format = 6,8 , not format = [6,8]. so far, i don't see any \"[]\"\n",
    "        #. characters in .nml files, so we can probably just get rid of them, but it might be smarter\n",
    "        #. to just recognize when we have a list type... or to enforce that all values are saved internally\n",
    "        #  as strings... The latter may become necessary, since there does not appear to be a good standard\n",
    "        #. for comma, space, etc. separating values (or fields).\n",
    "        '''\n",
    "        #\n",
    "        if json_in is None:\n",
    "            json_in = self\n",
    "        #\n",
    "        if indent is None:\n",
    "            indent = 4*chr(32)\n",
    "            #\n",
    "        #\n",
    "        if isinstance(json_in, str):\n",
    "            with open(json_in, 'r') as fin:\n",
    "                json_in = json.load(fin)\n",
    "            #\n",
    "        #\n",
    "        with open(nml_out, file_mode) as fout:\n",
    "            for group,entries in json_in.items():\n",
    "                fout.write('&{}\\n'.format(group))\n",
    "                #\n",
    "                for entry,val in entries.items():\n",
    "                    fout.write('{}{} = {}\\n'.format(indent, entry, val ))\n",
    "                fout.write('/\\n\\n')\n",
    "                \n",
    "                #\n",
    "            #\n",
    "            \n",
    "                    \n",
    "class NML_from_nml(NML):\n",
    "    def __init__(self, input_nml, output=None):\n",
    "        #\n",
    "        nml_out=output\n",
    "        super(NML_from_nml,self).__init__()\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        # is this useful?\n",
    "        #with open(input_nml) as fin:\n",
    "        #    self.nml = fin.read()\n",
    "        #\n",
    "        self.nml_to_json(input_nml, json_out=output)\n",
    "        \n",
    "    #\n",
    "class NML_from_json(NML):\n",
    "    def __init__(self, input_json, output=None):\n",
    "        #\n",
    "        json_out = output\n",
    "        super(NML_from_json,self).__init__()\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        with open(input_json) as fin:\n",
    "            #self.nml_dict = json.load(fin)\n",
    "            self.update(json.load(fin))\n",
    "        #\n",
    "        if not output is None:\n",
    "            self.nml = self.json_to_nml(self.nml_json, output)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** \n",
      "/Users/myoder96/Codes/AM4_runtime\n",
      "/Users/myoder96/Codes\n",
      "/Users/myoder96\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "root_dir = os.path.dirname(os.path.abspath(cwd))\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(\"..\"))\n",
    "#\n",
    "print('** \\n{}\\n{}\\n{}'.format(cwd, root_dir, ROOT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AM4_batch_scripter(object):\n",
    "    mpi_execs = {'mpirun':{'exec': 'mpirun', 'ntasks':'--np ', 'cpu_per_task':'-d '},\n",
    "                 'srun':{'exec':'srun', 'ntasks':'--ntasks=', 'cpu_per_task':'--cpus-per-task='}}\n",
    "    #\n",
    "    HPC_configs={\n",
    "    'mazama_hpc':{'cpu_per_node':24, 'cpu_slots':2, 'cpu_make':'intel', 'cpu_gen':'haswell',\n",
    "                  'mem_per_node':64, 'modules':['intel/19', 'openmpi_3/', 'gfdl_am4/']},\n",
    "    'sherlock2_hpc':{'cpu_per_node':24, 'cpu_slots':2, 'cpu_make':'intel', 'cpu_gen':'skylake',\n",
    "                     'mem_per_node':192},\n",
    "    'sherlock2_hpc2':{'cpu_per_node':24, 'cpu_slots':2, 'cpu_make':'intel', 'cpu_gen':'skylake',\n",
    "                      'mem_per_node':384},\n",
    "    'sherlock3_base':{'cpu_per_node':24, 'cpu_slots':1, 'cpu_make':'AMD', 'cpu_gen':'EPYC_7502',\n",
    "                      'mem_per_node':256},\n",
    "    'sherlock3_perf':{'cpu_per_node':128, 'cpu_slots':2, 'cpu_make':'AMD', 'cpu_gen':'EPYC_7742',\n",
    "                      'mem_per_node':1024}\n",
    "    }\n",
    "    #               \n",
    "    def __init__(self, batch_out='am4_batch.sh', work_dir=None, mpi_exec='mpirun',\n",
    "                 input_data_path=None, input_data_tar=None, input_data_url=None,\n",
    "                 nml_template='nml_input_template.nml', modules=None, diag_table_src='diag_table_v101',\n",
    "                 force_copy_input=0, do_tar=0, hpc_config='mazama_hpc',\n",
    "                 npes_atmos=48, nthreads_atmos=1, npes_ocean=0, job_name='AM4_run', sbatch_options_str='',\n",
    "                 copy_timeout=6000):\n",
    "        '''\n",
    "        # parameters? input data file?\n",
    "        #\n",
    "        # not sure what this looks like yet, but... This script/class will be called by a wrapper\n",
    "        #.  script. This process will constitute a step in a larer script (ie, each ~2 hour run in a\n",
    "        #. twohour queue process).\n",
    "        #. script\n",
    "        # 1) review, set up, etc. the working directory\n",
    "        # 2) Are the input data there?\n",
    "        # 3) if not, are the input data availble?\n",
    "        # 4) if not, is the tar available? if not, get it; then open, then copy.\n",
    "        # 5) evaluate the input/output data. Have we achieved our objectives\n",
    "        #.  (which we've not yet defined -- runtime, etc.)? Define restart as necessary \n",
    "        #.   (*** though actually, i guess this will be done by the calling script; this script will\n",
    "        #.    just receive instructions).\n",
    "        # 6) copy diag_table nd create input.nml\n",
    "        # 7) execute MPI command\n",
    "        #\n",
    "        # for Mazama (note: the future will probabl hold am4.json files like, am4_mazama.json, \n",
    "        #. am4_sherlock3.json, etc.)\n",
    "        #\n",
    "        # current and local root path(s):\n",
    "        #\n",
    "        # @n_tasks: I believe n_tasks needs to be an integer multiple of 6, ie 6 faces to a cube.\n",
    "        # (so any threads work on a single task/cube face). For now, let's stick with that, particularly\n",
    "        #. since we'll probably usually use nthreads=1\n",
    "        #. \n",
    "        '''\n",
    "        #        #\n",
    "        if isinstance(hpc_config, str):\n",
    "            if hpc_config.endswith('.json'):\n",
    "                hpc_config=json.load(hpc_config)\n",
    "            else:\n",
    "                hpc_config=self.HPC_configs[hpc_config]\n",
    "            #\n",
    "        # else hpc_config better be a dict of HW configs.\n",
    "        #\n",
    "        # \n",
    "        if npes_atmos%6 != 0:\n",
    "            npes_atmos += npes_atmos%6\n",
    "            print('*** WARNING: npes_atmos must be a multiple of 6. Increasing tasks -> {} so that npes_atmos%6=0'.format(npes_atmosn_tasks))\n",
    "        #\n",
    "        # same for ocean? npes_ocean%6==0 ??? right now, we're not doing ocean, so...\n",
    "        # TODO: we can also check to see that we don't ask for more threads than we can get, but since\n",
    "        #. these will almost always be single-threaded, let's hold off...\n",
    "        #\n",
    "        cwd = os.getcwd()\n",
    "        root_path = os.path.dirname(os.path.abspath(cwd))\n",
    "        default_data_path = os.path.join(root_path, 'AM4_data', 'AM4_run')\n",
    "        #\n",
    "        if input_data_path is None:\n",
    "            input_data_path = default_data_path\n",
    "        input_data_tar = (input_data_tar or os.path.join(os.path.dirname(input_data_path), 'AM4_run.tar.gz') )\n",
    "        if work_dir is None:\n",
    "            work_dir = os.path.join(cwd, 'workdir')\n",
    "        #\n",
    "        input_data_url = input_data_url or 'ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz'\n",
    "        # TODO: also download check validations:\n",
    "        # wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz.sha256\n",
    "        # wget ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz.sig\n",
    "        # sha256sum -c AM4_run.tar.gz.sha256\n",
    "        #gpg --verify AM4_run.tar.gz.sig\n",
    "        # \n",
    "        # TODO: How to handle the nml file? here as part of the batch, or in a wrapper that handles both?\n",
    "        #. or some combination\n",
    "        #\n",
    "        if modules is None:\n",
    "            modules = hpc_config.get('modules', ['intel/19', 'openmpi_3/', 'gfdl_am4/'])\n",
    "            #modules = ['intel/19', 'openmpi_3/', 'gfdl_am4/']\n",
    "        #\n",
    "        if input_data_path is None:\n",
    "            input_data_path = '{}/data/AM4_run'.format(root_path)\n",
    "        input_data_tar = input_data_tar or os.path.join(root_path, 'data', 'AM4_run.tar.gz')\n",
    "        #\n",
    "        if isinstance(mpi_exec, dict):\n",
    "            ky,vl = mpi_exec.items()[0]\n",
    "            #\n",
    "            # multi-category dict?\n",
    "            if isinstance(vl, dict):\n",
    "                self.mpi_execs.update(mpi_exec)\n",
    "                mpi_exec = vl\n",
    "            else:\n",
    "                # single-entry dict; add this full mpi_exec entry to mpi_execs\n",
    "                ky = mpi_exec['exec']\n",
    "                self.mpi_execs[ky] = mpi_exec.copy()\n",
    "            #\n",
    "        else:\n",
    "            mpi_exec = self.mpi_execs[mpi_exec]\n",
    "        #\n",
    "        #\n",
    "        # download, untar, copy input data:\n",
    "        \n",
    "        #\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in ('self', '__class__')})\n",
    "        #\n",
    "        layout_1 = get_layouts(self.ntasks_total)[0]\n",
    "        layout_io_1 = get_io_layouts(layout_1)[0]\n",
    "        layout_2 = get_layouts(self.ntasks_total/6)[0]\n",
    "        layout_io_2 = get_io_layouts(layout_2)[0]\n",
    "        self.__dict__.update({ky:vl for ky,vl in locals().items() if not ky in self.__dict__.keys()})\n",
    "        \n",
    "        # write an input.nml file and copy other files?\n",
    "        #\n",
    "        #\n",
    "        # write a batch file:\n",
    "        # self.write_batch_script()\n",
    "        \n",
    "    #\n",
    "    # NOTE: these nvcpu calculators will likely not work well for not-simple configurations,\n",
    "    #. but again, I think most of these jobs will be single-threaded.\n",
    "    @property\n",
    "    def cpus_total(self):\n",
    "        # NOTE: this is cpus, but not tasks. it should tell us how many cores we'll be expecting,\n",
    "        #. but probably is not something submitted to SLURM unless there are constraints on nodes.\n",
    "        return self.npes_atmos * self.nthreads_atmos + self.npes_ocean\n",
    "    @property\n",
    "    def ntasks_total(self):\n",
    "        return self.npes_atmos + self.npes_ocean\n",
    "    @property \n",
    "    def n_tasks(self):\n",
    "        return self.ntasks_total\n",
    "    @property\n",
    "    def n_threads(self):\n",
    "        return self.nthreads_atmos\n",
    "    #\n",
    "    #def layout_to_str(self, layout):\n",
    "    #    return '{},{}'.format(*layout)\n",
    "    #\n",
    "    @property\n",
    "    def default_nml_reconfig(self):\n",
    "        # TODO: check these. if we're actually using ocean... is ocean supposet to be n*x=npes_ocean?\n",
    "        #.  npes_total???\n",
    "#         return {'coupler_nml':{'atmos_npes':self.npes_atmos, 'atmos_nthreads':self.nthreads_atmos,\n",
    "#                               'ocean_npes':self.npes_ocean},\n",
    "#                'fv_core_nml':{'layout':self.layout_2, 'io_layout':self.layout_io_2},\n",
    "#                'ice_model_nml':{'layout':self.layout_1, 'io_layout':self.layout_io_1},\n",
    "#                'land_model_nml':{'layout':self.layout_2, 'io_layout':self.layout_io_2},\n",
    "#                'ocean_model_nml':{'layout':self.layout_1, 'io_layout':self.layout_io_1},\n",
    "#                }\n",
    "        return {'coupler_nml':{'atmos_npes':self.npes_atmos, 'atmos_nthreads':self.nthreads_atmos,\n",
    "                              'ocean_npes':self.npes_ocean},\n",
    "               'fv_core_nml':{'layout':'{},{}'.format(*self.layout_2),\n",
    "                              'io_layout':'{},{}'.format(*self.layout_io_2)},\n",
    "               'ice_model_nml':{'layout':'{},{}'.format(*self.layout_1),\n",
    "                              'io_layout':'{},{}'.format(*self.layout_io_1)},\n",
    "               'land_model_nml':{'layout':'{},{}'.format(*self.layout_2),\n",
    "                              'io_layout':'{},{}'.format(*self.layout_io_2)},\n",
    "               'ocean_model_nml':{'layout':'{},{}'.format(*self.layout_1),\n",
    "                              'io_layout':'{},{}'.format(*self.layout_io_1)},\n",
    "                'atmos_model_nml':{'nxblocks':1, 'nyblocks':self.nthreads_atmos}\n",
    "               }\n",
    "    #\n",
    "    def get_input_data(self, work_dir=None, input_dir=None, input_tar=None, input_data_url=None,\n",
    "                       force_copy=None, verbose=0, copy_timeout=None, diag_table_src=None):\n",
    "        if work_dir is None:\n",
    "            work_dir = self.work_dir\n",
    "        force_copy = force_copy or self.force_copy_input\n",
    "        input_dir = input_dir or self.input_data_path\n",
    "        input_tar = input_tar or self.input_data_tar\n",
    "        input_data_url = input_data_url or self.input_data_url\n",
    "        copy_timeout = copy_timeout or self.copy_timeout\n",
    "        diag_table_src = diag_table_src or self.diag_table_src\n",
    "        #\n",
    "        # root-dir for tar file:\n",
    "        if not os.path.isdir(os.path.dirname(input_tar)):\n",
    "            os.makedirs(os.path.dirname(input_tar))\n",
    "        #\n",
    "        # It is possible we might create work_dir in advance to add some files, but not copy all the data,\n",
    "        #. so let's use INPUT as our test case.\n",
    "        # TODO: better tests to (not) copy\n",
    "        #input_dir = os.path.join(work_dir, 'INPUT')\n",
    "        #\n",
    "        print('*** DEBUG: os.path.isdir(os.path.join(work_dir, \\'INPUT\\'))={}'.format(os.path.isdir(os.path.join(work_dir, 'INPUT'))))\n",
    "        #\n",
    "       # what is a minimum file-count for INPUT? the default input config is ~500 files.\n",
    "        if not os.path.isdir(work_dir):\n",
    "            os.makedirs(work_dir)\n",
    "        #\n",
    "        if not os.path.isdir(os.path.join(work_dir, 'INPUT')) or len(os.listdir(os.path.join(work_dir, 'INPUT')))<100 or force_copy:\n",
    "            # work_dir is not there or not complete, so\n",
    "            # we're going to copy data from the input source to the workdir.\n",
    "            # Do we have an input source?\n",
    "            #\n",
    "            # TODO: check for isdir(.input_path), then for .tar here.\n",
    "            if not os.path.isdir(input_dir):\n",
    "                # input data are not available; go get them!\n",
    "                #\n",
    "                if verbose:\n",
    "                    print('*** VERBOSE: os.path.isfile(input_tar)={}'.format(os.path.isfile(input_tar)))\n",
    "                if not os.path.isfile(input_tar):\n",
    "                    # No TAR file avilble; we'll have to get it...\n",
    "                    # TODO: consider downloading data first, then opening and writing file to mitigate\n",
    "                    #. opportunities for contamination.\n",
    "                    if verbose: print('*** VERBOSE: fetching data from {}'.format(input_data_url))\n",
    "                    #\n",
    "                    # NOTE: requests. does not support ftp, so use urllib or urllib2\n",
    "                    with contextlib.closing(urllib.request.urlopen(input_data_url)) as url_fin:\n",
    "                        with open(input_tar, 'wb') as fout:\n",
    "                            shutil.copyfileobj(url_fin, fout)\n",
    "                #\n",
    "                if verbose:\n",
    "                    print('*** VERBOSE: untarring * {} * to: {} '.format(input_tar, input_dir))\n",
    "                with tarfile.open(input_tar, 'r:gz') as tar:\n",
    "                    # this is another one of those cases where a shell call might be easier...\n",
    "                    tar.extractall(path=os.path.dirname(input_dir))\n",
    "            #\n",
    "            if verbose:\n",
    "                print('*** VERBOSE: copying input data from {} to {}'.format(input_dir, work_dir))\n",
    "            #\n",
    "            # Nuke existing workdir? for now, let's not...\n",
    "            # TODO: shutil.copy2() should work for this, but it's being tempermental, maybe about the\n",
    "            #. directory. So for now, let's just subprocess the shell command and revisit later. It could\n",
    "            #. be a versioing problem (subprocess and related libraries are pretty version squirrelie).\n",
    "            #src = input_dir\n",
    "            #dst = work_dir\n",
    "            # TODO: The Python copy options just don't seem to work very well (maybe there's a recursive option command or \n",
    "            #. something to tell it we're copying a directory?). Let's just use a shell command.\n",
    "            #shutil.copy2(input_dir, work_dir)\n",
    "            cp_command = 'cp -rf {}/ {}/'.format(input_dir, work_dir)\n",
    "            try:\n",
    "                # NOTE: beware the timeout.... I think the default timeout is 60 seconds or something;\n",
    "                #. this can take a while, so I've been using 6000 seconds as a default.\n",
    "                #sp_output = subprocess.run(cp_command.split(chr(32)), check=True,\n",
    "                #           stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "                sp_output = subprocess.run(cp_command.split(chr(32)), check=True,\n",
    "                           capture_output=True, timeout=copy_timeout)\n",
    "            except:\n",
    "                #print('*** copy error: stdout: {}, sterr: {}'.format(sp_output.stdout, sp_output=stderr) )\n",
    "                print('*** COPY ERROR: {}'.format(cp_command))\n",
    "                raise Exception('subprocess, copy error.')\n",
    "            #\n",
    "            if verbose:\n",
    "                print('*** VERBOSE: copy input data complete, from {} to {}'.format(input_dir, work_dir))\n",
    "            #\n",
    "            # copy diag_table from a source file. We're going to construct an input.nml separately.\n",
    "            # NOTE: Again, might be better to do this as a shell command, for a number of reasons.\n",
    "            shutil.copy(diag_table_src, os.path.append(work_dir, 'diag_table'))\n",
    "            #\n",
    "        if not os.path.isdir(os.path.join(work_dir, 'RESTART')):\n",
    "            os.makedirs(os.path.join(work_dir, 'RESTART'))\n",
    "        #\n",
    "    #\n",
    "    def make_NML(self, nml_template=None, nml_configs=[], nml_out=None, json_out=None):\n",
    "        '''\n",
    "        # Some logic to managing and writing nml output.\n",
    "        # @nml_template: An nml file or .json as a starter place.\n",
    "        # @ nml_configs: 1 or more dicts (allow json filenames??) with configuration(s); they will be\n",
    "        #.  integrated, in order, via dict.update(nml_config)\n",
    "        #\n",
    "        '''\n",
    "        # add any default configs:\n",
    "        # NOTE: putting default_nml_reconfig first allows the user to override.\n",
    "        nml_configs = [self.default_nml_reconfig] + list(nml_configs)\n",
    "        nml_template = nml_template or self.nml_template\n",
    "        \n",
    "        #\n",
    "        if nml_template.endswith('.nml'):\n",
    "            f_handler = NML_from_nml\n",
    "            #NML = NML_from_nml(nml_template, output=None)\n",
    "        elif nml_template.endswith('.json'):\n",
    "            #NML = NML_from_json(nml_template, output=None)\n",
    "            f_handler = NML_from_json\n",
    "            \n",
    "        else:\n",
    "            # guess?\n",
    "            # TODO: crack it open and look for .nml or\n",
    "            #NML = NML_from_nml(nml_template, json_out=None)\n",
    "            with open(nml_template, 'r') as fin:\n",
    "                f_handler = NML_from_json\n",
    "                for rw in fin:\n",
    "                    if rw.startswith('&coupler_nml'):\n",
    "                        # it is probably a .nml file.\n",
    "                        f_handler = NML_from_nml\n",
    "                        break\n",
    "                    #\n",
    "                #\n",
    "        NML = f_handler(nml_template, None)\n",
    "            #\n",
    "        #\n",
    "        for config in nml_configs:\n",
    "            #\n",
    "            if isinstance(config, str):\n",
    "                with open(config, 'r') as fin:\n",
    "                    config = json.load(config)\n",
    "                #\n",
    "            #\n",
    "            #NML.update(config)\n",
    "            for ky,d in config.items():\n",
    "                NML[ky].update(d)\n",
    "            #\n",
    "        #\n",
    "        if not nml_out is None:\n",
    "            NML.json_to_nml(nml_out=nml_out)\n",
    "        if not json_out is None:\n",
    "            with open(json_out, 'w') as fout:\n",
    "                json.dump(NML, fout)\n",
    "            #\n",
    "        self.NML=NML\n",
    "        #\n",
    "        return NML  \n",
    "    #\n",
    "    def write_batch_script(self, fname_out=None, chdir=None, output_out=None, output_err=None,\n",
    "                           mpi_exec=None):\n",
    "        '''\n",
    "        # Just as it sounds; write an AM4 batch script. use various inputs (prams, json, dicts?);\n",
    "        #. fetch, untar, copy input data as necessary, etc.\n",
    "        # Let's start by just scripting it out (mostly), and then we'll decide what to read in as\n",
    "        #. input prams, what to read in from JSON (or something), etc. Note that, for now, this will\n",
    "        #  focus on a SE3 Mazama build/configuration.\n",
    "        '''\n",
    "        #\n",
    "        fname_out = fname_out or os.path.join(self.work_dir, self.batch_out)\n",
    "        chdir = chdir or self.work_dir\n",
    "        output_out = output_out or 'AM4_out_%j.out'\n",
    "        output_err = output_err or 'AM4_out_%j.err'\n",
    "        mpi_exec = mpi_exec or self.mpi_exec\n",
    "        #\n",
    "        with open(fname_out, 'w') as fout:\n",
    "            # This section should be pretty universal:\n",
    "            fout.write('#!/bin/bash\\n#\\n')\n",
    "            #\n",
    "            fout.write('#SBATCH --ntasks={}\\n'.format(self.n_tasks))\n",
    "            fout.write('#SBATCVH --ncpus_per_task={}\\n'.format(self.nthreads_atmos))\n",
    "            if not self.job_name is None or self.job_name=='':\n",
    "                fout.write('#SBATCH --job-name={}\\n'.format(self.job_name))\n",
    "            #\n",
    "            fout.write('#SBATCH --chdir={}\\n'.format(chdir) )\n",
    "            fout.write('#SBATCH --output={}\\n'.format(output_out))\n",
    "            fout.write('#SBATCH --error={}\\n'.format(output_err))\n",
    "            #\n",
    "            # Module swill be platform dependent. We could handle this section with JSON\n",
    "            #. if we wanted to.\n",
    "            fout.write('#\\nmodule purge\\n#\\n')\n",
    "            #\n",
    "            for m in self.modules:\n",
    "                fout.write('module load {}\\n'.format(m))\n",
    "            #\n",
    "            fout.write('#\\n#\\n')\n",
    "            # Universal??? My guess is we can impprove performance by increasing these values on bigger\n",
    "            #. memory machines.\n",
    "            # these probably need to be parameterized somehow, or included in a template... or otherwise\n",
    "            #. we need to figure out what they should be. I just copied them from GFDL sample scripts.\n",
    "            fout.write('export KMP_STACKSIZE=512m\\n')\n",
    "            fout.write('export NC_BLKSZ=1M\\n')\n",
    "            fout.write('export F_UFMTENDIAN=big\\n')\n",
    "            #\n",
    "            # executable will be pretty system/build dependent\n",
    "            fout.write('EXECUTABLE=${AM4_GFDL_BIN}/${AM4_GFDL_EXE}\\n')\n",
    "            #\n",
    "            # We can probably make this part pretty universal as well...\n",
    "            fout.write('#\\nulimit -s unlimited\\n#\\n')\n",
    "            fout.write('#\\ncd {}\\n#\\n'.format(self.work_dir))\n",
    "            fout.write('MPI_COMMAND={} {}{} {}{} ${{EXECUTABLE}}\\n#\\n'.format(mpi_exec['exec'],\n",
    "                                                            mpi_exec['ntasks'], self.n_tasks,\n",
    "                                                            mpi_exec['cpu_per_task'], self.n_threads))\n",
    "            #\n",
    "            # add an error-check:\n",
    "            for ln in ['if [[ %? -ne 0 ]]; then', 'echo \"ERROR: Run failed.\" 1>&2', \n",
    "                       '\"ERROR: Output from run in {}/fms.out ... or maybe in a log file\" 1>&2'.format(self.work_dir),\n",
    "                       'exit 1', 'fi' ]:\n",
    "                fout.write('{}\\n'.format(ln))\n",
    "            #\n",
    "        #\n",
    "            \n",
    "    #\n",
    "def get_layouts(n_tasks=24):\n",
    "    '''\n",
    "    # compute possible layouts. Include (some) error checking for valid n_tasks?\n",
    "    '''\n",
    "    # get all integer factor pairs:\n",
    "    return numpy.array(sorted([(k,int(n_tasks/k)) for k in range(1, int(numpy.ceil(n_tasks**.5)))\n",
    "                               if n_tasks%k==0],\n",
    "                                key = lambda rw: numpy.sum(rw)))\n",
    "#\n",
    "def get_io_layouts(layout):\n",
    "    '''\n",
    "    # AM4 io_layouts. the second \"y\" term must be an integer factor of the \"y\" term of the input layout.\n",
    "    #. Don't yet understand the first term, so for now let's limit it to 1.\n",
    "    '''\n",
    "    #\n",
    "    return numpy.array(sorted([(1,int(layout[1]/k)) for k in range(1, int(numpy.ceil(layout[1]**.5)))\n",
    "                               if layout[1]%k==0], \n",
    "                              key=lambda rw:numpy.sum(rw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a string with SOME in {$braces}\n"
     ]
    }
   ],
   "source": [
    "print('a string with {} in {{$braces}}'.format('SOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** ABS variables:\n",
      "batch_out: am4_batch.sh\n",
      "work_dir: /Users/myoder96/Codes/AM4_runtime/workdir2\n",
      "mpi_exec: {'exec': 'mpirun', 'ntasks': '--np ', 'cpu_per_task': '-d '}\n",
      "input_data_path: /Users/myoder96/Codes/AM4_data2/AM4_run\n",
      "input_data_tar: /Users/myoder96/Codes/AM4_data2/AM4_run.tar.gz\n",
      "input_data_url: ftp://nomads.gfdl.noaa.gov/users/Ming.Zhao/AM4Documentation/GFDL-AM4.0/inputData/AM4_run.tar.gz\n",
      "nml_template: nml_input_template.nml\n",
      "modules: ['intel/19', 'openmpi_3/', 'gfdl_am4/']\n",
      "diag_table_src: diag_table_v101\n",
      "force_copy_input: 0\n",
      "do_tar: 0\n",
      "hpc_config: {'cpu_per_node': 24, 'cpu_slots': 2, 'cpu_make': 'intel', 'cpu_gen': 'haswell', 'mem_per_node': 64, 'modules': ['intel/19', 'openmpi_3/', 'gfdl_am4/']}\n",
      "npes_atmos: 48\n",
      "nthreads_atmos: 1\n",
      "npes_ocean: 0\n",
      "job_name: AM4_dev\n",
      "sbatch_options_str: \n",
      "copy_timeout: 6000\n",
      "cwd: /Users/myoder96/Codes/AM4_runtime\n",
      "root_path: /Users/myoder96/Codes\n",
      "default_data_path: /Users/myoder96/Codes/AM4_data/AM4_run\n",
      "layout_1: [6 8]\n",
      "layout_io_1: [1 4]\n",
      "layout_2: [2 4]\n",
      "layout_io_2: [1 4]\n",
      "self: <__main__.AM4_batch_scripter object at 0x7fae22014a20>\n",
      "*** DEBUG: os.path.isdir(os.path.join(work_dir, 'INPUT'))=True\n"
     ]
    }
   ],
   "source": [
    "#work_dir = \n",
    "ABS = AM4_batch_scripter(input_data_path='/Users/myoder96/Codes/AM4_data2/AM4_run', \n",
    "                         work_dir='/Users/myoder96/Codes/AM4_runtime/workdir2', job_name='AM4_dev')\n",
    "#\n",
    "print('*** ABS variables:')\n",
    "for key,val in ABS.__dict__.items():\n",
    "    print('{}: {}'.format(key,val))\n",
    "#\n",
    "\n",
    "zz = ABS.get_input_data(verbose=True)\n",
    "\n",
    "\n",
    "my_configs = {'coupler_nml':{'days':10, 'months':0}, 'fv_core_nml':{'npx':193, 'npy':193, 'npz':50}}\n",
    "my_nml = ABS.make_NML(nml_template='input_yoder_v101.nml', nml_configs=[my_configs],\n",
    "                      nml_out=os.path.join(ABS.work_dir, 'input.nml') )\n",
    "# NML_from_nml('input_yoder_v101.nml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** my_nml[fv_core_nml]:\n",
      "**  {'layout': '2,4', 'io_layout': '1,4', 'npx': 193, 'npy': 193, 'ntiles': '6,', 'npz': 50, 'k_split': '1,', 'n_split': '12,', 'a2b_ord': '4,', 'adjust_dry_mass': '.true.,', 'adj_mass_vmr': '.true.,', 'print_freq': '0,', 'grid_type': '0,', 'tau': '0.', 'do_uni_zfull': '.true.', 'n_sponge': '0', 'd2_bg_k1': '0.16', 'd2_bg_k2': '0.02', 'kord_tm': '-10', 'kord_mt': '10', 'kord_tr': '10', 'hydrostatic': '.T.', 'd_ext': '0.', 'd2_bg': '0.', 'nord': '2', 'dddmp': '0.', 'd4_bg': '0.15', 'vtdm4': '0.0', 'do_vort_damp': '.F.', 'd_con': '0.', 'hord_mt': '10', 'hord_vt': '10', 'hord_tm': '10', 'hord_dp': '10', 'hord_tr': '8', 'consv_te': '0.7', 'consv_am': '.T.', 'fill': '.T.', 'z_tracer': '.T.'}\n",
      "** batch_out:  am4_batch.sh\n"
     ]
    }
   ],
   "source": [
    "#my_nml = NML_from_nml('input_yoder_v101.nml')\n",
    "print('** my_nml[fv_core_nml]:' )\n",
    "print('** ', my_nml['fv_core_nml'])\n",
    "#print(my_nml.keys())\n",
    "\n",
    "print('** batch_out: ', ABS.batch_out)\n",
    "#\n",
    "ABS.write_batch_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exec': 'mpirun', 'ntasks': '--np ', 'cpu_per_task': '-d '}\n",
      "MPI_COMMAND=mpirun --np 48 -d 1 ${EXECUTABLE\\}\n"
     ]
    }
   ],
   "source": [
    "print(ABS.mpi_exec)\n",
    "\n",
    "print('MPI_COMMAND={} {}{} {}{} ${{EXECUTABLE\\}}'.format(ABS.mpi_exec['exec'],\n",
    "                                                            ABS.mpi_exec['ntasks'], ABS.n_tasks,\n",
    "                                                            ABS.mpi_exec['cpu_per_task'], ABS.n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** sp_output: \n",
      " CompletedProcess(args=['ls', '-lh', '/Users/myoder96/Codes/AM4_data2/AM4_run/'], returncode=0, stdout=b'total 352\\ndrwxr-xr-x  502 myoder96  staff    16K Jul 14  2017 INPUT\\ndrwxr-xr-x    2 myoder96  staff    64B Jul 17  2017 RESTART\\n-rw-r--r--    1 myoder96  staff   283B Jul 14  2017 data_table\\n-rw-r--r--    1 myoder96  staff    45K Jul 14  2017 diag_table\\n-rw-r--r--    1 myoder96  staff    11K Jul 14  2017 field_table\\n-rw-r--r--    1 myoder96  staff    55K Jul 14  2017 input.nml\\n-rw-r--r--    1 myoder96  staff    55K Jul 14  2017 input.nml.unexpanded\\n')\n",
      "** ** stdout: \n",
      " total 352\n",
      "drwxr-xr-x  502 myoder96  staff    16K Jul 14  2017 INPUT\n",
      "drwxr-xr-x    2 myoder96  staff    64B Jul 17  2017 RESTART\n",
      "-rw-r--r--    1 myoder96  staff   283B Jul 14  2017 data_table\n",
      "-rw-r--r--    1 myoder96  staff    45K Jul 14  2017 diag_table\n",
      "-rw-r--r--    1 myoder96  staff    11K Jul 14  2017 field_table\n",
      "-rw-r--r--    1 myoder96  staff    55K Jul 14  2017 input.nml\n",
      "-rw-r--r--    1 myoder96  staff    55K Jul 14  2017 input.nml.unexpanded\n",
      "\n",
      "**  total 352\n",
      "**  drwxr-xr-x  502 myoder96  staff    16K Jul 14  2017 INPUT\n",
      "**  drwxr-xr-x    2 myoder96  staff    64B Jul 17  2017 RESTART\n",
      "**  -rw-r--r--    1 myoder96  staff   283B Jul 14  2017 data_table\n",
      "**  -rw-r--r--    1 myoder96  staff    45K Jul 14  2017 diag_table\n",
      "**  -rw-r--r--    1 myoder96  staff    11K Jul 14  2017 field_table\n",
      "**  -rw-r--r--    1 myoder96  staff    55K Jul 14  2017 input.nml\n",
      "**  -rw-r--r--    1 myoder96  staff    55K Jul 14  2017 input.nml.unexpanded\n",
      "**  \n"
     ]
    }
   ],
   "source": [
    "# simplified syntax to capture both stdout and stderr (to separate outputs?). NOTE: this syntax has been\n",
    "#. a moving target\n",
    "#sp_output = subprocess.run('ls -lh'.split(chr(32)), check=True, capture_output=True)\n",
    "#\n",
    "# pipe stdout and stderr to the same output\n",
    "sp_output = subprocess.run('ls -lh /Users/myoder96/Codes/AM4_data2/AM4_run/'.split(chr(32)), check=True,\n",
    "                           stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "#\n",
    "print('** sp_output: \\n', sp_output)\n",
    "# for rw in sp_output.stdout.decode().split('\\n'):\n",
    "#     print*('** ', rw)\n",
    "print('** ** stdout: \\n', sp_output.stdout.decode())\n",
    "for rw in sp_output.stdout.decode().split('\\n'):\n",
    "    print('** ', rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** coupler_nml:{'months': '1,', 'days': '0,', 'current_date': '1979,1,1,0,0,0,', 'calendar': \"'julian'\", 'dt_atmos': '1800,', 'dt_cpld': '7200,', 'use_lag_fluxes': '.true.', 'concurrent': '.false.', 'do_ocean': '.false.', 'ocean_npes': '0', 'atmos_npes': '48', 'atmos_nthreads': '1', 'use_hyper_thread': '.false.', 'ncores_per_node': '24'}\n",
      "\n",
      "*** coupler_nml:{'months': '1,', 'days': '0,', 'current_date': '1979,1,1,0,0,0,', 'calendar': \"'julian'\", 'dt_atmos': '1800,', 'dt_cpld': '7200,', 'use_lag_fluxes': '.true.', 'concurrent': '.false.', 'do_ocean': '.false.', 'ocean_npes': '0', 'atmos_npes': '48', 'atmos_nthreads': '1', 'use_hyper_thread': '.false.', 'ncores_per_node': '24'}\n",
      "\n",
      "*** vegn_data_nml:{'vegn_to_use': \"'uniform'\", 'K1': '10,', 'K2': '0.1,', 'fsc_liv': '0.9,', 'fsc_wood': '0.45,', 'c1(4)': '0.3', 'c2(4)': '0.3', 'Vmax': '2.0E-5, 2.0E-5, 2.0E-5, 2.0E-5, 1.50E-5,', 'm_cond': '4., 9., 9., 7., 7.,', 'alpha_phot': '0.05, 0.06, 0.06, 0.06, 0.06,', 'gamma_resp': '0.03, 0.02, 0.02, 0.02, 0.02,', 'tc_crit(0:2)': '3*273.16', 'fact_crit_phen(0:4)': '0., 0., 0., 0., 0.', 'fact_crit_fire(0:4)': '0., 0., 0., 0., 0.', 'cnst_crit_phen(0:4)': '0.30, 0.15, 0.15, 0.30, 0.30', 'cnst_crit_fire(0:4)': '0.15,  0.40, 0.15, 0.15, 0.15', 'wet_leaf_dreg(0:4)': '.3, .3, .3, .3, .3', 'ksi': '0, 0, 0, 0, 0,', 'leaf_refl(0:4,1)': '0.11, 0.11, 0.10, 0.10, 0.10', 'leaf_refl(0:4,2)': '0.58, 0.58, 0.45, 0.45, 0.39,', 'dat_root_zeta(0:4)': '0.35212, 0.17039, 0.28909, 0.25813, 0.17039', 'critical_root_density': '0.0,', 'tau_drip_s': '259200.0', 'cmc_lai(0:4)': '0.02, 0.02, 0.02, 0.02, 0.02', 'csc_lai(0:4)': '0.30, 0.30, 0.30, 0.30, 0.60', 'dat_snow_crit': '4*1.e7, .1', 't_transp_min': '268.', 'srl(0:1)': '112.0e3, 150.0e3', 'root_perm': '14*5e-7', 'alpha(1,3)': '4', 'leaf_age_tau(2)': '150', 'smoke_fraction': '0.9, 0.9, 0.6, 0.6, 0.6', 'tg_c3_thresh': '1', 'phen_ev1': '0.2', 'phen_ev2': '0.7', 'cmc_eps': '0.01', 'alpha(0:4,6)': '0.0, 0.0, 0.012, 0.012, 0.012', 'treefall_disturbance_rate': '0.175, 0.185, 0.025, 0.0275, 0.027'}\n",
      "\n",
      "*** simple_sulfate_nml:{'gas_conc_filename': \"'gas_conc_3D_am3p9.nc'\", 'gas_conc_time_dependency_type': \"'constant'\", 'cont_volc_source': \"'do_cont_volc'\", 'expl_volc_source': \"'do_expl_volc'\", 'aerocom_emission_filename': \"'so2_0.25_volcanoes.nc'\", 'aircraft_source': \"'do_aircraft',\", 'aircraft_filename': \"'emissions.aircraft.aero.0.5x0.5.1849-2016.nc',\", 'aircraft_emission_name(1)': \"'fuel'\", 'aircraft_time_dependency_type': \"'time_varying'\", 'aircraft_dataset_entry': '1979 1 1 0 0 0', 'so2_aircraft_EI': '0.001', 'anthro_source': \"'do_anthro',\", 'anthro_emission_name(1)': \"'so2ff',\", 'anthro_emission_name(2)': \"'so4ff',\", 'anthro_time_dependency_type': \"'time_varying'\", 'anthro_dataset_entry': '1979 1 1 0 0 0', 'anthro_filename': \"'anthro_so2.1849_2016.nc',\", 'biobur_source': \"'do_biobur',\", 'biobur_emission_name(1)': \"'so2bb',\", 'biobur_emission_name(2)': \"'so4bb',\", 'biobur_time_dependency_type': \"'time_varying'\", 'biobur_dataset_entry': '1979 1 1 0 0 0', 'biobur_filename': \"'anthro_so2.1849_2016.nc',\", 'cloud_chem_solver': '\"f1p\"', 'pH_cloud': '4.5', 'no_biobur_if_no_pbl': '.false.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NML_test = NML_from_nml('input_yoder_v101.nml')\n",
    "#\n",
    "\n",
    "# for ky,vl in JJ.nml_dict.items():\n",
    "#     print('** {}: {}'.format(ky,vl))\n",
    "\n",
    "\n",
    "for ky in ['coupler_nml','coupler_nml', 'vegn_data_nml', 'simple_sulfate_nml' ]:\n",
    "    print('*** {}:{}\\n'.format(ky, NML_test[ky]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NML_test['aerosolrad_package_nml']['sulfate_indices'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working NML example.\n",
    "\n",
    "- Start with a standard template\n",
    "- Compute layouts for an MPI configuration\n",
    "- Modify layout variables (in internal JSON/dict)\n",
    "- Export working input.nml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Layouts_1:  [[ 6  8]\n",
      " [ 4 12]\n",
      " [ 3 16]\n",
      " [ 2 24]\n",
      " [ 1 48]]\n",
      "** Layouts_2:  [[2 4]\n",
      " [1 8]]\n",
      "** Layouts_io_1:  [[1 4]\n",
      " [1 8]]\n",
      "** Layouts_io_2:  [[1 4]]\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "n_tasks = 48\n",
    "n_threads = 1\n",
    "#\n",
    "layouts_1 = get_layouts(n_tasks)\n",
    "layouts_2 = get_layouts(n_tasks = n_tasks/6)\n",
    "#\n",
    "layout_io_1 = get_io_layouts(layouts_1[0])\n",
    "layout_io_2 = get_io_layouts(layouts_2[0])\n",
    "#\n",
    "print('** Layouts_1: ', layouts_1)\n",
    "print('** Layouts_2: ', layouts_2)\n",
    "#\n",
    "print('** Layouts_io_1: ', layout_io_1)\n",
    "print('** Layouts_io_2: ', layout_io_2)\n",
    "#\n",
    "my_nml = NML_from_nml(input_nml='input_yoder_v101.nml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***  dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "print('*** ', my_nml.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Layouts_1:  [[ 6  8]\n",
      " [ 4 12]\n",
      " [ 3 16]\n",
      " [ 2 24]\n",
      " [ 1 48]]\n",
      "** Layouts_2:  [[2 4]\n",
      " [1 8]]\n",
      "** Layouts_io_1:  [[1 4]\n",
      " [1 8]]\n",
      "** Layouts_io_2:  [[1 4]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fv_core_nml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-2838194df9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# print out layouts, as they are imported:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'fv_core_nml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'land_model_nml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ocean_model_nml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ice_model_nml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'** {}::layout: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_nml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'** {}::io_layout: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_nml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fv_core_nml'"
     ]
    }
   ],
   "source": [
    "layout_1=layouts_1[0]\n",
    "layout_2=layouts_2[0]\n",
    "#\n",
    "layout_io = layout_io_1[0]\n",
    "\n",
    "#\n",
    "# print out layouts, as they are imported:\n",
    "for grp in ('fv_core_nml', 'land_model_nml','ocean_model_nml', 'ice_model_nml'):\n",
    "    print('** {}::layout: {}'.format(grp, my_nml[grp]['layout']))\n",
    "    print('** {}::io_layout: {}'.format(grp, my_nml[grp]['layout']))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ', 48%6)\n",
    "print('** ', 48%5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('** ', my_nml['fv_core_nml']['layout'])\n",
    "#\n",
    "for grp in ('fv_core_nml', 'land_model_nml'):\n",
    "    my_nml.assign(grp, 'layout', ','.join([str(x) for x in layout_2]))\n",
    "    my_nml.assign(grp, 'io_layout', ','.join([str(x) for x in layout_io]))\n",
    "    print('** {}:: {}, {}'.format(grp, my_nml[grp]['layout'], my_nml[grp]['io_layout']))\n",
    "#\n",
    "for grp in ('ocean_model_nml', 'ice_model_nml'):\n",
    "    my_nml.assign(grp, 'layout', ','.join([str(x) for x in layout_1]))\n",
    "    my_nml.assign(grp, 'io_layout', ','.join([str(x) for x in layout_io]))\n",
    "    print('** {}:: {}, {}'.format(grp, my_nml[grp]['layout'], my_nml[grp]['io_layout']))\n",
    "#\n",
    "for ky,vl in [('npx',193), ('npy', 193), ('npz', 50)]:\n",
    "    my_nml.assign('fv_core_nml', ky, vl)\n",
    "#\n",
    "for ky,vl in [('co2_ceiling', 4800.0E-06), ('time_varying_co2', '.true.'), ('co2_base_value',348.0E-06),\n",
    "            ('co2_floor', 100.0E-06), ('c02_data_source', 'namelist') ]:\n",
    "    # NOTE: we want to allow new assignment:\n",
    "    my_nml['radiative_gases_nml'][ky] = vl\n",
    "print('*** radiative_gases: ', my_nml['radiative_gases_nml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ky,vl in my_nml['radiative_gases_nml'].items():\n",
    "    print('** {}:: {}'.format(ky, vl))\n",
    "print('\\n\\n')\n",
    "#\n",
    "for ky,vl in my_nml['fv_core_nml'].items():\n",
    "    print('** {}:: {}'.format(ky, vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nml.json_to_nml(nml_out='my_output.nml', json_in=my_nml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(','.join([str(x) for x in [1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
